{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nimport pandas as pd\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport csv\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-12T11:41:16.024683Z","iopub.execute_input":"2023-05-12T11:41:16.025342Z","iopub.status.idle":"2023-05-12T11:41:25.369413Z","shell.execute_reply.started":"2023-05-12T11:41:16.025307Z","shell.execute_reply":"2023-05-12T11:41:25.368456Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"val_df = pd.read_csv(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_valid.csv\", header=None)\ntrain_df = pd.read_csv(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_train.csv\", header=None)\ntest_df = pd.read_csv(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_test.csv\", header=None)\nprint(\"Data Loaded to Dataframes!\")","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:25.371383Z","iopub.execute_input":"2023-05-12T11:41:25.372011Z","iopub.status.idle":"2023-05-12T11:41:25.543725Z","shell.execute_reply.started":"2023-05-12T11:41:25.371971Z","shell.execute_reply":"2023-05-12T11:41:25.542700Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Data Loaded to Dataframes!\n","output_type":"stream"}]},{"cell_type":"code","source":"tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_train.csv\")\nread_tsv = csv.reader(tsv_file)\nval_tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_valid.csv\")\nval_read_tsv = csv.reader(val_tsv_file)\ntest_tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_test.csv\")\ntest_read_tsv = csv.reader(test_tsv_file)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:25.544985Z","iopub.execute_input":"2023-05-12T11:41:25.545620Z","iopub.status.idle":"2023-05-12T11:41:25.553369Z","shell.execute_reply.started":"2023-05-12T11:41:25.545584Z","shell.execute_reply":"2023-05-12T11:41:25.552510Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_X = []\ntrain_Y = []\ntest_X = []\ntest_Y = []\nval_X = []\nval_Y = []\nfor i in read_tsv:   \n    train_Y.append(i[1])\n    train_X.append(i[0])\nfor i in val_read_tsv:\n    val_Y.append(i[1])\n    val_X.append(i[0])\nfor i in test_read_tsv:\n    test_Y.append(i[1])\n    test_X.append(i[0])\n\ntest_Y = np.array(test_Y)\ntest_X = np.array(test_X)\nfor i in range(test_Y.shape[0]):\n    test_Y[i] = \"\\t\" + test_Y[i] + \"\\n\"\ntrain_Y = np.array(train_Y)\ntrain_X = np.array(train_X)\nfor i in range(train_Y.shape[0]):\n    train_Y[i] = \"\\t\" + train_Y[i] + \"\\n\"\nval_Y = np.array(val_Y)\nval_X = np.array(val_X)\nfor i in range(val_Y.shape[0]):\n    val_Y[i] = \"\\t\" + val_Y[i] + \"\\n\"","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:25.556119Z","iopub.execute_input":"2023-05-12T11:41:25.556661Z","iopub.status.idle":"2023-05-12T11:41:25.756156Z","shell.execute_reply.started":"2023-05-12T11:41:25.556610Z","shell.execute_reply":"2023-05-12T11:41:25.755205Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_X","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:51:50.030764Z","iopub.execute_input":"2023-05-12T05:51:50.031111Z","iopub.status.idle":"2023-05-12T05:51:50.042732Z","shell.execute_reply.started":"2023-05-12T05:51:50.031080Z","shell.execute_reply":"2023-05-12T05:51:50.041645Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array(['vargaalavaarine', 'vastadira', 'factamfos', ...,\n       'venakkiteesukoovaalane', 'roopaantaraalu', 'chendindindi'],\n      dtype='<U28')"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:51:50.272705Z","iopub.execute_input":"2023-05-12T05:51:50.273912Z","iopub.status.idle":"2023-05-12T05:51:50.291866Z","shell.execute_reply.started":"2023-05-12T05:51:50.273870Z","shell.execute_reply":"2023-05-12T05:51:50.290942Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:51:50.473694Z","iopub.execute_input":"2023-05-12T05:51:50.476250Z","iopub.status.idle":"2023-05-12T05:51:50.596943Z","shell.execute_reply.started":"2023-05-12T05:51:50.476210Z","shell.execute_reply":"2023-05-12T05:51:50.596051Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:51:50.685086Z","iopub.execute_input":"2023-05-12T05:51:50.687329Z","iopub.status.idle":"2023-05-12T05:51:50.702238Z","shell.execute_reply.started":"2023-05-12T05:51:50.687291Z","shell.execute_reply":"2023-05-12T05:51:50.701065Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(test_Y)\nprint(test_X)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:25.757757Z","iopub.execute_input":"2023-05-12T11:41:25.758163Z","iopub.status.idle":"2023-05-12T11:41:25.763929Z","shell.execute_reply.started":"2023-05-12T11:41:25.758126Z","shell.execute_reply":"2023-05-12T11:41:25.762794Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['\\tవిత్తనాన్ని\\n' '\\tప్రయాణికులు\\n' '\\tహసన్\\n' ... '\\tతెలంగాణ\\n'\n '\\tపటేల్\\n' '\\tపేడ\\n']\n['vithananni' 'prayaanikulu' 'hassan' ... 'telamgaanha' 'patel' 'peda']\n","output_type":"stream"}]},{"cell_type":"code","source":"# english_characters = set()\n# devnagri_characters = set()\ninput_corpus = set()\noutput_corpus = set()\nfor word in train_X:\n    for char in word:\n        if char not in input_corpus:\n            input_corpus.add(char)\n\nfor word in train_Y:\n    for char in word:\n        if char not in output_corpus:\n            output_corpus.add(char)\n\n# Validation set\n# v_english_characters = set()\n# v_devnagri_characters = set()\nval_input_corpus = set()\nval_output_corpus = set()\n\nfor word in val_X:\n    for char in word:\n        if char not in val_input_corpus:\n            val_input_corpus.add(char)\n\nfor word in val_Y:\n    for char in word:\n        if char not in val_output_corpus:\n            val_output_corpus.add(char)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:31.938783Z","iopub.execute_input":"2023-05-12T11:41:31.939182Z","iopub.status.idle":"2023-05-12T11:41:32.204226Z","shell.execute_reply.started":"2023-05-12T11:41:31.939147Z","shell.execute_reply":"2023-05-12T11:41:32.203308Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"input_corpus.add(\" \")\noutput_corpus.add(\" \")\ninput_corpus = sorted(list(input_corpus))\noutput_corpus = sorted(list(output_corpus))\nnum_encoder_tokens = len(input_corpus)\nnum_decoder_tokens = len(output_corpus)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:32.205961Z","iopub.execute_input":"2023-05-12T11:41:32.206385Z","iopub.status.idle":"2023-05-12T11:41:32.211724Z","shell.execute_reply.started":"2023-05-12T11:41:32.206353Z","shell.execute_reply":"2023-05-12T11:41:32.210676Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"max_encoder_seq_length = max([len(txt) for txt in train_X]) + 2\nmax_decoder_seq_length = max([len(txt) for txt in train_Y])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:32.485189Z","iopub.execute_input":"2023-05-12T11:41:32.485871Z","iopub.status.idle":"2023-05-12T11:41:32.552975Z","shell.execute_reply.started":"2023-05-12T11:41:32.485835Z","shell.execute_reply":"2023-05-12T11:41:32.552079Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"Number of samples:\", len(train_X))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:32.919726Z","iopub.execute_input":"2023-05-12T11:41:32.920079Z","iopub.status.idle":"2023-05-12T11:41:32.927695Z","shell.execute_reply.started":"2023-05-12T11:41:32.920045Z","shell.execute_reply":"2023-05-12T11:41:32.926643Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Number of samples: 51200\nNumber of unique input tokens: 27\nNumber of unique output tokens: 65\nMax sequence length for inputs: 30\nMax sequence length for outputs: 21\n","output_type":"stream"}]},{"cell_type":"code","source":"hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\nhindi_alphabet_size = len(hindi_alphabets)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:33.198729Z","iopub.execute_input":"2023-05-12T11:41:33.199061Z","iopub.status.idle":"2023-05-12T11:41:33.203776Z","shell.execute_reply.started":"2023-05-12T11:41:33.199025Z","shell.execute_reply":"2023-05-12T11:41:33.202520Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"input_char_index = dict([(char, i) for i, char in enumerate(input_corpus)])\noutput_char_index = dict([(char, i) for i, char in enumerate(output_corpus)])\n\ninput_data = np.zeros((max_encoder_seq_length,len(train_X)), dtype=\"int64\")\ntarget_data = np.zeros((max_decoder_seq_length,len(train_X)), dtype=\"int64\")\n\nfor i, (x, y) in enumerate(zip(train_X, train_Y)):\n    for t, char in enumerate(x):\n        input_data[t, i] = input_char_index[char]\n        \n    input_data[t + 1 :,i] = input_char_index[\" \"]\n    \n    for t, char in enumerate(y):\n        target_data[t, i] = output_char_index[char]\n            \n    target_data[t + 1 :,i] = output_char_index[\" \"]\n    \ninput_data_val = np.zeros((max_encoder_seq_length,len(val_X)), dtype=\"int64\")\ntarget_data_val = np.zeros((max_decoder_seq_length,len(val_X)), dtype=\"int64\")\n\n\nfor i, (x, y) in enumerate(zip(val_X, val_Y)):\n    for t, char in enumerate(x):\n        input_data_val[t, i] = input_char_index[char]\n        \n    input_data_val[t + 1 :,i] = input_char_index[\" \"]\n    \n    for t, char in enumerate(y):\n        target_data_val[t, i] = output_char_index[char]\n            \n    target_data_val[t + 1 :,i] = output_char_index[\" \"]","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:34.589465Z","iopub.execute_input":"2023-05-12T11:41:34.590190Z","iopub.status.idle":"2023-05-12T11:41:35.557747Z","shell.execute_reply.started":"2023-05-12T11:41:34.590151Z","shell.execute_reply":"2023-05-12T11:41:35.556748Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# convertin numpy arrays to tensors\ninput_data = torch.tensor(input_data,dtype=torch.int64)\ntarget_data = torch.tensor(target_data,dtype=torch.int64)\ninput_data_val = torch.tensor(input_data_val,dtype=torch.int64)\ntarget_data_val = torch.tensor(target_data_val,dtype=torch.int64)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:35.559751Z","iopub.execute_input":"2023-05-12T11:41:35.560119Z","iopub.status.idle":"2023-05-12T11:41:35.610526Z","shell.execute_reply.started":"2023-05-12T11:41:35.560085Z","shell.execute_reply":"2023-05-12T11:41:35.609565Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#LSTM RUN Only\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (seq_length, N, embedding_size)\n\n        outputs, (hidden, cell) = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n\n        return hidden, cell\nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden, cell):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (1, N, embedding_size)\n\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        # outputs shape: (1, N, hidden_size)\n\n        predictions = self.fc(outputs)\n\n        # predictions shape: (1, N, length_target_vocabulary) to send it to\n        # loss function we want it to be (N, length_target_vocabulary) so we're\n        # just gonna remove the first dim\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.2):\n        batch_size = source.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = num_decoder_tokens\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n        hidden, cell = self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n        x = target[0]\n\n        for t in range(1, target_len):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden, cell = self.decoder(x, hidden, cell)\n\n            # Store next output prediction\n            outputs[t] = output\n\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else best_guess\n\n        return outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:35.612103Z","iopub.execute_input":"2023-05-12T11:41:35.612610Z","iopub.status.idle":"2023-05-12T11:41:35.628459Z","shell.execute_reply.started":"2023-05-12T11:41:35.612576Z","shell.execute_reply":"2023-05-12T11:41:35.627507Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\nlearning_rate = 0.001\nbatch_size = 32\nload_model = False\ninput_size_encoder = num_encoder_tokens\ninput_size_decoder = num_decoder_tokens\noutput_size = num_decoder_tokens\nencoder_embedding_size = 256\ndecoder_embedding_size = 256\nhidden_size = 256  # Needs to be the same for both RNN's\nnum_enc_layers = 2\nnum_dec_layers = 2\nenc_dropout = 0.1\ndec_dropout = 0.1\ntraining = False","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:36.679571Z","iopub.execute_input":"2023-05-12T11:41:36.679946Z","iopub.status.idle":"2023-05-12T11:41:36.685982Z","shell.execute_reply.started":"2023-05-12T11:41:36.679915Z","shell.execute_reply":"2023-05-12T11:41:36.684992Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#LSTM RUN Only\nencoder_net = Encoder(\n    input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    num_dec_layers,\n    dec_dropout,\n).to(device)\n\nmodel = Seq2Seq(encoder_net, decoder_net).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:36.872068Z","iopub.execute_input":"2023-05-12T11:41:36.872583Z","iopub.status.idle":"2023-05-12T11:41:43.832279Z","shell.execute_reply.started":"2023-05-12T11:41:36.872552Z","shell.execute_reply":"2023-05-12T11:41:43.831258Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_ds_x = torch.split(input_data,batch_size,dim=1)\ntrain_ds_y = torch.split(target_data,batch_size,dim=1)\ninput_data_val = input_data_val.to(device)\ntarget_data_val = target_data_val.to(device)\ntarget_val = target_data_val[1:].reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:43.833978Z","iopub.execute_input":"2023-05-12T11:41:43.834325Z","iopub.status.idle":"2023-05-12T11:41:43.856376Z","shell.execute_reply.started":"2023-05-12T11:41:43.834292Z","shell.execute_reply":"2023-05-12T11:41:43.855567Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# this cell is only for training, not to be used now as we have saved the model\n\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    model.eval()\n    model.train()\n\n    for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n        # Get input and targets and get to cuda\n        inp_data = x.to(device)\n        target = y.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n        # way that we have output_words * batch_size that we want to send in into\n        # our cost function, so we need to do some reshapin. While we're at it\n        # Let's also remove the start token while we're at it\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n        \n        \n#torch.save(model.state_dict(),'models\\model_pytorch_noAT_state.pt')\n#torch.save(model,'models\\model_pytorch_noAT.pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:41:43.857740Z","iopub.execute_input":"2023-05-12T11:41:43.858112Z","iopub.status.idle":"2023-05-12T11:43:33.354034Z","shell.execute_reply.started":"2023-05-12T11:41:43.858054Z","shell.execute_reply":"2023-05-12T11:43:33.353058Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[Epoch 0 / 2]\n[Epoch 1 / 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:43:33.358186Z","iopub.execute_input":"2023-05-12T11:43:33.358548Z","iopub.status.idle":"2023-05-12T11:43:33.366237Z","shell.execute_reply.started":"2023-05-12T11:43:33.358515Z","shell.execute_reply":"2023-05-12T11:43:33.365183Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Seq2Seq(\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (embedding): Embedding(27, 256)\n    (rnn): LSTM(256, 256, num_layers=2, dropout=0.1)\n  )\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (embedding): Embedding(65, 256)\n    (rnn): LSTM(256, 256, num_layers=2, dropout=0.1)\n    (fc): Linear(in_features=256, out_features=65, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict((i, char) for char, i in input_char_index.items())\nreverse_target_char_index = dict((i, char) for char, i in output_char_index.items())","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:43:33.367813Z","iopub.execute_input":"2023-05-12T11:43:33.368199Z","iopub.status.idle":"2023-05-12T11:43:33.378463Z","shell.execute_reply.started":"2023-05-12T11:43:33.368163Z","shell.execute_reply":"2023-05-12T11:43:33.377507Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def translate(model, word, input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device):\n    \n    word_t = ''\n    data = np.zeros((max_encoder_seq_length,1), dtype=\"int64\")\n    for t, char in enumerate(word):\n        data[t, 0] = input_char_index[char]\n        \n    data[t + 1 :,0] = input_char_index[\" \"]\n    \n    data = torch.tensor(data,dtype=torch.int64).to(device)\n\n    with torch.no_grad():\n        hidden, cell = model.encoder(data)\n        \n    x = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n\n    for t in range(1, max_decoder_seq_length):\n        output, hidden, cell = model.decoder(x, hidden, cell)\n        print(output.shape)\n        best_guess = output.argmax(1)\n        x = best_guess\n        ch = reverse_target_char_index[x.item()]\n        if ch == '\\n':\n            break\n        else:\n            word_t = word_t+ch\n\n    return word_t","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:43:33.454280Z","iopub.execute_input":"2023-05-12T11:43:33.454532Z","iopub.status.idle":"2023-05-12T11:43:33.463678Z","shell.execute_reply.started":"2023-05-12T11:43:33.454509Z","shell.execute_reply":"2023-05-12T11:43:33.462284Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"total_words = len(train_X)\ncorrect_pred = 0\nfor i in range(total_words):\n    #print(train_Y[i])\n    decoded_sentence = translate(model,train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device)\n#    print(translate(model, train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n             # reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n             # num_encoder_tokens, num_decoder_tokens, device))\n    #print(decoded_sentence)\n    if train_Y[i][1:-1]== decoded_sentence:\n        correct_pred += 1\n       # print('True')\ntest_accuracy = correct_pred / total_words\n\nprint(test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T10:16:32.893067Z","iopub.status.idle":"2023-05-12T10:16:32.894136Z","shell.execute_reply.started":"2023-05-12T10:16:32.893845Z","shell.execute_reply":"2023-05-12T10:16:32.893892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words = len(val_X)\ncorrect_pred = 0\nfor i in range(20):\n    #print(train_Y[i])\n    decoded_sentence = translate(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device)\n#    print(translate(model, train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n             # reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n             # num_encoder_tokens, num_decoder_tokens, device))\n    #print(decoded_sentence)\n    if val_Y[i][1:-1]== decoded_sentence:\n        correct_pred += 1\n       # print('True')\ntest_accuracy = correct_pred / total_words\n\nprint(test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:43:33.464845Z","iopub.execute_input":"2023-05-12T11:43:33.465275Z","iopub.status.idle":"2023-05-12T11:43:33.573228Z","shell.execute_reply.started":"2023-05-12T11:43:33.465247Z","shell.execute_reply":"2023-05-12T11:43:33.572159Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"torch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\ntorch.Size([1, 65])\n0.001220703125\n","output_type":"stream"}]},{"cell_type":"code","source":"total_words = len(test_X)\ncorrect_pred = 0\nfor i in range(total_words):\n    #print(test_Y[i][1:-1])\n    decoded_sentence = translate(model,test_X[i], input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device)\n#    print(translate(model, train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n             # reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n             # num_encoder_tokens, num_decoder_tokens, device))\n    #print(decoded_sentence)\n    #print('\\n')\n    if test_Y[i][1:-1]== decoded_sentence:\n        correct_pred += 1\n       # print('True')\ntest_accuracy = correct_pred / total_words\n\nprint(test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\nlearning_rate = 0.001\nbatch_size = 32\nload_model = False\ninput_size_encoder = num_encoder_tokens\ninput_size_decoder = num_decoder_tokens\noutput_size = num_decoder_tokens\nencoder_embedding_size = 256\ndecoder_embedding_size = 256\nhidden_size = 256  # Needs to be the same for both RNN's\nnum_enc_layers = 2\nnum_dec_layers = 2\nenc_dropout = 0.1\ndec_dropout = 0.1\ntraining = False","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:52:20.141433Z","iopub.execute_input":"2023-05-12T05:52:20.142484Z","iopub.status.idle":"2023-05-12T05:52:20.148632Z","shell.execute_reply.started":"2023-05-12T05:52:20.142447Z","shell.execute_reply":"2023-05-12T05:52:20.147492Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def training(num_encoder_tokens,input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers,num_epochs,output_size,input_size_decoder):\n    encoder_net = Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp).to(device)\n    decoder_net = Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n\n    model = Seq2Seq(encoder_net, decoder_net).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    train_ds_x = torch.split(input_data,batch_size,dim=1)\n    train_ds_y = torch.split(target_data,batch_size,dim=1)\n    #print(train_ds_x)\n    for epoch in range(num_epochs):\n        print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n        model.eval()\n        model.train()\n\n        for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n        # Get input and targets and get to cuda\n            inp_data = x.to(device)\n            target = y.to(device)\n\n            # Forward prop\n            output = model(inp_data, target)\n\n\n            output = output[1:].reshape(-1, output.shape[2])\n            target = target[1:].reshape(-1)\n\n            optimizer.zero_grad()\n            loss = criterion(output, target)\n\n            # Back prop\n            loss.backward()\n\n            # Clip to avoid exploding gradient issues, makes sure grads are\n            # within a healthy range\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n            # Gradient descent step\n            optimizer.step()\n        total_words = len(val_X)\n        correct_pred = 0\n        model.eval()\n        for i in range(total_words):\n       # print(val_Y[i][1:-1])\n            decoded_sentence = translate(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n                      reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n                      num_encoder_tokens, num_decoder_tokens, device)\n            if val_Y[i][1:-1]== decoded_sentence:\n                 correct_pred += 1\n        #print(decoded_sentence)\n        #print('\\n')\n        test_accuracy = correct_pred / total_words\n\n        print(test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:52:21.968548Z","iopub.execute_input":"2023-05-12T05:52:21.968897Z","iopub.status.idle":"2023-05-12T05:52:21.992256Z","shell.execute_reply.started":"2023-05-12T05:52:21.968866Z","shell.execute_reply":"2023-05-12T05:52:21.991227Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"training(input_size_encoder ,256, 0.1, \"LSTM\", 256, 2, 2,20,num_decoder_tokens,num_decoder_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:52:24.142778Z","iopub.execute_input":"2023-05-12T05:52:24.143448Z","iopub.status.idle":"2023-05-12T06:08:55.657747Z","shell.execute_reply.started":"2023-05-12T05:52:24.143405Z","shell.execute_reply":"2023-05-12T06:08:55.656194Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[Epoch 0 / 20]\n0.082763671875\n[Epoch 1 / 20]\n0.276123046875\n[Epoch 2 / 20]\n0.35693359375\n[Epoch 3 / 20]\n0.380126953125\n[Epoch 4 / 20]\n0.415283203125\n[Epoch 5 / 20]\n0.436767578125\n[Epoch 6 / 20]\n0.443603515625\n[Epoch 7 / 20]\n0.46240234375\n[Epoch 8 / 20]\n0.455810546875\n[Epoch 9 / 20]\n0.47509765625\n[Epoch 10 / 20]\n0.46923828125\n[Epoch 11 / 20]\n0.4716796875\n[Epoch 12 / 20]\n0.48388671875\n[Epoch 13 / 20]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size_encoder\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_decoder_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_decoder_tokens\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[39], line 23\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(num_encoder_tokens, input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers, num_epochs, output_size, input_size_decoder)\u001b[0m\n\u001b[1;32m     20\u001b[0m target \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward prop\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     27\u001b[0m target \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[31], line 71\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, target_len):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Use previous hidden, cell as context from encoder at start\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     output, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Store next output prediction\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     outputs[t] \u001b[38;5;241m=\u001b[39m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1495\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1495\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n    'parameters': {'embedding_size': {'values': [128, 256, 512]},\n                   'hidden_size': {'values': [128, 256, 512]},\n                   'cell_type': {'values': ['LSTM']},\n                   'num_layers': {'values': [1,2,3]},\n                   'batch_size': {'values': [128,256,512]},\n                   'dropout': {'values': [0.1, 0.2, 0.3, 0.4]},\n                   'epochs' = :{'values':[10,20,30,40]}\n                }}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_config = {\n#     'method': 'bayes',\n#     'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n#     'parameters': {'input_embedding_size': {'values': [128, 256, 512]},\n#                    'hidden_layer_size': {'values': [128, 256, 512]},\n#                    'cell_type': {'values': ['LSTM', 'RNN', 'GRU']},\n#                    'num_layers': {'values': [1,2,3]},\n#                    'batch_size': {'values': [128,256,512]},\n#                    'dropout': {'values': [0.1, 0.2, 0.3, 0.4]}\n#                 }}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    var1 = wandb.init()\n    var2 = var1.config\n    epochs = 10\n\n    model, encoder_layers, decoder_layers = training(var2.input_embedding_size, var2.dropout, var2.cell_type , var2.hidden_layer_size, var2.num_layers, var2.num_layers)\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    model.fit(\n        [encoder_input_data, decoder_input_data],\n        decoder_target_data,\n        batch_size=var2.batch_size,\n        epochs=epochs,\n        callbacks=[WandbCallback()]\n    )\n\n    encoder_model, decoder_model = inferencing(model,var2.num_layers, var2.num_layers,encoder_layers,decoder_layers,var2.cell_type,var2.hidden_layer_size)\n    correct = 0\n    n = val_devnagri.shape[0]\n    for i in range(n):\n        input = encoder_val_input_data[i:i+1]\n        output = decode_sequence(input,encoder_model, decoder_model)\n        if output.strip() == val_devnagri[i].strip():\n            correct += 1\n    wandb.log({'val_accuracy' : correct*100/n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## This cell is to Run RNN\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(EncoderRNN, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (seq_length, N, embedding_size)\n        outputs, hidden = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n        # hidden shape: (num_layers, N, hidden_size)\n        return hidden\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-12T06:11:36.705308Z","iopub.execute_input":"2023-05-12T06:11:36.705742Z","iopub.status.idle":"2023-05-12T06:11:36.712765Z","shell.execute_reply.started":"2023-05-12T06:11:36.705708Z","shell.execute_reply":"2023-05-12T06:11:36.711803Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"##For RNN\nclass DecoderRNN(nn.Module):\n    def __init__(\n        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n        super(DecoderRNN, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, output_size)\n      #  self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x, hidden):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (1, N, embedding_size)\n\n        outputs, hidden = self.rnn(embedding, hidden)\n        predictions = self.fc(outputs)\n     #   predictions = self.softmax(predictions)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:10:43.159231Z","iopub.execute_input":"2023-05-12T09:10:43.159622Z","iopub.status.idle":"2023-05-12T09:10:43.167340Z","shell.execute_reply.started":"2023-05-12T09:10:43.159588Z","shell.execute_reply":"2023-05-12T09:10:43.166400Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"#Defining the complete model for RNN\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = source.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = num_decoder_tokens\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n        hidden = self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n        x = target[0]\n\n        for t in range(1, target_len):\n            # Use previous hidden, cell as context from encoder at start\n           # output, hidden, cell = self.decoder(x, hidden, cell)\n            output, hidden= self.decoder(x, hidden)\n            # Store next output prediction\n            outputs[t] = output\n\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n\n            # With probability of teacher_force_ratio we take the actual next word\n            # otherwise we take the word that the Decoder predicted it to be.\n            # Teacher Forcing is used so that the model gets used to seeing\n            # similar inputs at training and testing time, if teacher forcing is 1\n            # then inputs at test time might be completely different than what the\n            # network is used to. This was a long comment.\n            x = target[t] if random.random() < teacher_force_ratio else best_guess\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:28:15.272232Z","iopub.execute_input":"2023-05-12T09:28:15.272674Z","iopub.status.idle":"2023-05-12T09:28:15.282102Z","shell.execute_reply.started":"2023-05-12T09:28:15.272636Z","shell.execute_reply":"2023-05-12T09:28:15.281215Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"#ENN RUN Only\nencoder_net = EncoderRNN(\n    input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout\n).to(device)\n\ndecoder_net = DecoderRNN(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    num_dec_layers,\n    dec_dropout\n).to(device)\n\nmodel = Seq2Seq(encoder_net, decoder_net).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:28:16.237463Z","iopub.execute_input":"2023-05-12T09:28:16.238211Z","iopub.status.idle":"2023-05-12T09:28:16.251613Z","shell.execute_reply.started":"2023-05-12T09:28:16.238173Z","shell.execute_reply":"2023-05-12T09:28:16.250579Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_ds_x = torch.split(input_data,batch_size,dim=1)\ntrain_ds_y = torch.split(target_data,batch_size,dim=1)\ninput_data_val = input_data_val.to(device)\ntarget_data_val = target_data_val.to(device)\ntarget_val = target_data_val[1:].reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:28:16.805251Z","iopub.execute_input":"2023-05-12T09:28:16.805665Z","iopub.status.idle":"2023-05-12T09:28:16.822891Z","shell.execute_reply.started":"2023-05-12T09:28:16.805631Z","shell.execute_reply":"2023-05-12T09:28:16.821870Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"# this cell is only for training, not to be used now as we have saved the model\n\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    model.eval()\n    model.train()\n\n    for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n        # Get input and targets and get to cuda\n        inp_data = x.to(device)\n        target = y.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n        # way that we have output_words * batch_size that we want to send in into\n        # our cost function, so we need to do some reshapin. While we're at it\n        # Let's also remove the start token while we're at it\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n        \n        \n#torch.save(model.state_dict(),'models\\model_pytorch_noAT_state.pt')\n#torch.save(model,'models\\model_pytorch_noAT.pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:28:17.084174Z","iopub.execute_input":"2023-05-12T09:28:17.084561Z","iopub.status.idle":"2023-05-12T09:31:49.173426Z","shell.execute_reply.started":"2023-05-12T09:28:17.084502Z","shell.execute_reply":"2023-05-12T09:31:49.171281Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"[Epoch 0 / 10]\n[Epoch 1 / 10]\n[Epoch 2 / 10]\n[Epoch 3 / 10]\n[Epoch 4 / 10]\n[Epoch 5 / 10]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[163], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m target \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Forward prop\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# doesn't take input in that form. For example if we have MNIST we want to have\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# output to be: (N, 10) and targets just (N). Here we can view it in a similar\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# way that we have output_words * batch_size that we want to send in into\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# our cost function, so we need to do some reshapin. While we're at it\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Let's also remove the start token while we're at it\u001b[39;00m\n\u001b[1;32m     24\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[160], line 23\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, target_len):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Use previous hidden, cell as context from encoder at start\u001b[39;00m\n\u001b[1;32m     22\u001b[0m    \u001b[38;5;66;03m# output, hidden, cell = self.decoder(x, hidden, cell)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     output, hidden\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Store next output prediction\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     outputs[t] \u001b[38;5;241m=\u001b[39m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[133], line 23\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     20\u001b[0m    embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x))\n\u001b[1;32m     21\u001b[0m    \u001b[38;5;66;03m# embedding shape: (1, N, embedding_size)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m    outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m    predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(outputs)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#   predictions = self.softmax(predictions)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:509\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    514\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    515\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict((i, char) for char, i in input_char_index.items())\nreverse_target_char_index = dict((i, char) for char, i in output_char_index.items())","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:31:49.174548Z","iopub.status.idle":"2023-05-12T09:31:49.175902Z","shell.execute_reply.started":"2023-05-12T09:31:49.175634Z","shell.execute_reply":"2023-05-12T09:31:49.175660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate(model, word, input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device):\n    \n    word_t = ''\n    data = np.zeros((max_encoder_seq_length,1), dtype=\"int64\")\n    for t, char in enumerate(word):\n        data[t, 0] = input_char_index[char]\n        \n    data[t + 1 :,0] = input_char_index[\" \"]\n    \n    data = torch.tensor(data,dtype=torch.int64).to(device)\n\n    with torch.no_grad():\n        hidden = model.encoder(data)\n        \n    x = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n\n    for t in range(1, max_decoder_seq_length):\n        output, hidden = model.decoder(x, hidden)\n        best_guess = output.argmax(1)\n        x = best_guess\n        ch = reverse_target_char_index[x.item()]\n        if ch == '\\n':\n            break\n        else:\n            word_t = word_t+ch\n\n    return word_t","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:31:58.732540Z","iopub.execute_input":"2023-05-12T09:31:58.733249Z","iopub.status.idle":"2023-05-12T09:31:58.743658Z","shell.execute_reply.started":"2023-05-12T09:31:58.733212Z","shell.execute_reply":"2023-05-12T09:31:58.742459Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"# #LSTM Beam Search\n# def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,\n#                 reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n#                 num_encoder_tokens, num_decoder_tokens, beam_width, device):\n\n#     word_t = ''\n\n#     # Encode the input word\n#     data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n#     for t, char in enumerate(word):\n#         data[t, 0] = input_char_index[char]\n\n#     data[t + 1:, 0] = input_char_index[\" \"]\n\n#     data = torch.tensor(data, dtype=torch.int64).to(device)\n\n#     with torch.no_grad():\n#         hidden,cell = model.encoder(data)\n\n#     # Initialize beam\n#     initial_sequence = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n#     beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n\n#     for _ in range(max_decoder_seq_length):\n#         candidates = []\n#         for score, seq, hidden in beam:\n#             last_token = seq[-1].item()\n#             if last_token == output_char_index['\\n']:\n#                 # If the sequence ends with the end token, add it to the candidates\n#                 candidates.append((score, seq, hidden))\n#                 continue\n\n#             x = torch.tensor(np.array(last_token).reshape(1,)).to(device)\n#             output, hidden,cell = model.decoder(x, hidden.squeeze(0),cell)\n#             probabilities = F.softmax(output, dim=1)\n\n#             # Get the top-k probabilities and tokens\n#             topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n\n#             for prob, token in zip(topk_probs[0], topk_tokens[0]):\n#                 new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n#                 new_hidden = hidden.clone().unsqueeze(0)\n#                 candidates.append((score + torch.log(prob).item(), new_seq, new_hidden))\n\n#         # Select top-k candidates based on the accumulated scores\n#         beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n#     # Select the best sequence from the beam as the output\n#     best_score, best_sequence, _ = beam[0]\n#     word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n\n#     return word_t","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\nlearning_rate = 0.01\nbatch_size = 32\nload_model = False\ninput_size_encoder = num_encoder_tokens\ninput_size_decoder = num_decoder_tokens\noutput_size = num_decoder_tokens\nencoder_embedding_size = 128\ndecoder_embedding_size = 128\nhidden_size = 128  # Needs to be the same for both RNN's\nnum_enc_layers = 1\nnum_dec_layers = 1\nenc_dropout = 0.1\ndec_dropout = 0.1\ntraining = False","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:31:49.179902Z","iopub.status.idle":"2023-05-12T09:31:49.180806Z","shell.execute_reply.started":"2023-05-12T09:31:49.180492Z","shell.execute_reply":"2023-05-12T09:31:49.180540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words = len(val_X)\ncorrect_pred = 0\nfor i in range(total_words):\n    #print(val_Y[i][1:-1])\n    decoded_sentence = translate(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device)\n#    print(translate(model, train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n             # reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n             # num_encoder_tokens, num_decoder_tokens, device))\n   # print(decoded_sentence)\n    #print('\\n')\n    if val_Y[i][1:-1]== decoded_sentence:\n        correct_pred += 1\n       # print('True')\ntest_accuracy = correct_pred / total_words\n\nprint(test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:32:06.527620Z","iopub.execute_input":"2023-05-12T09:32:06.528657Z","iopub.status.idle":"2023-05-12T09:32:25.992728Z","shell.execute_reply.started":"2023-05-12T09:32:06.528610Z","shell.execute_reply":"2023-05-12T09:32:25.991689Z"},"trusted":true},"execution_count":165,"outputs":[{"name":"stdout","text":"0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,\n#               reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n#               num_encoder_tokens, num_decoder_tokens, beam_width, device):\n\n#     word_t = ''\n\n#     # Encode the input word\n#     data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n#     for t, char in enumerate(word):\n#         data[t, 0] = input_char_index[char]\n\n#     data[t + 1:, 0] = input_char_index[\" \"]\n\n#     data = torch.tensor(data, dtype=torch.int64).to(device)\n\n#     with torch.no_grad():\n#         hidden = model.encoder(data)\n\n#     # Initialize beam\n#     beam = [(torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device), hidden, 0)]  # [(sequence, hidden, score)]\n\n#     x = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n#     for _ in range(max_decoder_seq_length):\n#         candidates = []\n#         for seq, hidden, score in beam:\n#             last_token = seq[-1]\n#             output, hidden = model.decoder(x, hidden)\n#             probabilities = F.softmax(output, dim=1)\n\n#             # Expand the beam\n#             topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n#             for prob, token in zip(topk_probs[0], topk_tokens[0]):\n#                 new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n#                 new_hidden = hidden\n#                 new_score = score + prob.item()\n#                 candidates.append((new_seq, new_hidden, new_score))\n\n#         # Select top-k candidates\n#         candidates.sort(key=lambda x: x[2], reverse=True)\n#         beam = candidates[:beam_width]\n\n#         # Check if the sequences in the beam have reached the end token\n#         end_token_idx = output_char_index['\\n']\n#         for seq, _, _ in beam:\n#             if seq[-1].item() == end_token_idx:\n#                 # If the end token is reached, select the sequence as the output\n#                 word_t = ''.join([reverse_target_char_index[token.item()] for token in seq[1:-1]])\n#                 return word_t\n\n#     # If the maximum sequence length is reached, select the best sequence in the beam as the output\n#     best_sequence = beam[0][0]\n#     word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n\n#     return word_t\n# def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,\n#               reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n#               num_encoder_tokens, num_decoder_tokens, beam_width, device):\n\n#     word_t = ''\n\n#     # Encode the input word\n#     data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n#     for t, char in enumerate(word):\n#         data[t, 0] = input_char_index[char]\n\n#     data[t + 1:, 0] = input_char_index[\" \"]\n\n#     data = torch.tensor(data, dtype=torch.int64).to(device)\n\n#     with torch.no_grad():\n#         hidden = model.encoder(data)\n\n#     # Initialize beam\n#     beam = [(torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device), hidden, 0)]  # [(sequence, hidden, score)]\n\n#     x = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n#     for _ in range(max_decoder_seq_length):\n#         candidates = []\n#         for seq, hidden, score in beam:\n#             last_token = seq[-1]\n#             output, hidden = model.decoder(x, hidden)\n#             probabilities = F.softmax(output, dim=1)\n\n#             # Expand the beam\n#             topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n#             for prob, token in zip(topk_probs[0], topk_tokens[0]):\n#                 new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n#                 new_hidden = hidden\n#                 new_score = score + torch.log(prob).item()\n#                 candidates.append((new_seq, new_hidden, new_score))\n\n#         # Select top-k candidates\n#         candidates.sort(key=lambda x: x[2] / len(x[0]), reverse=True)\n#         beam = candidates[:beam_width]\n\n#         # Check if the sequences in the beam have reached the end token\n#         end_token_idx = output_char_index['\\n']\n#         for seq, _, _ in beam:\n#             if seq[-1].item() == end_token_idx:\n#                 # If the end token is reached, select the sequence as the output\n#                 word_t = ''.join([reverse_target_char_index[token.item()] for token in seq[1:-1]])\n#                 return word_t\n\n#     # If the maximum sequence length is reached, select the best sequence in the beam as the output\n#     best_sequence = beam[0][0]\n#     word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n\n#     return word_t\n# def beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,\n#               reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n#               num_encoder_tokens, num_decoder_tokens, beam_width, device):\n\n#     word_t = ''\n\n#     # Encode the input word\n#     data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n#     for t, char in enumerate(word):\n#         data[t, 0] = input_char_index[char]\n\n#     data[t + 1:, 0] = input_char_index[\" \"]\n\n#     data = torch.tensor(data, dtype=torch.int64).to(device)\n\n#     with torch.no_grad():\n#         hidden = model.encoder(data)\n\n#     # Initialize beam\n#     beam = [(torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device), hidden, 0)]  # [(sequence, hidden, score)]\n\n#     x = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n#     for _ in range(max_decoder_seq_length):\n#         candidates = []\n#         for seq, hidden, score in beam:\n#             last_token = seq[-1]\n#             output, hidden = model.decoder(x, hidden)\n#             probabilities = F.softmax(output, dim=1)\n\n#             # Expand the beam\n#             topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n#             for prob, token in zip(topk_probs[0], topk_tokens[0]):\n#                 new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n#                 new_hidden = hidden.clone().unsqueeze(0)  # Update the hidden state\n#                 new_score = score + torch.log(prob).item()\n#                 candidates.append((new_seq, new_hidden, new_score))\n\n# #         # Select top-k candidates\n#         candidates.sort(key=lambda x: x[2] / len(x[0]), reverse=True)\n#         beam = candidates[:beam_width]\n\n# #         # Check if the sequences in the beam have reached the end token\n# #         end_token_idx = output_char_index['\\n']\n# #         for seq, _, _ in beam:\n# #             if seq[-1].item() == end_token_idx:\n# #                 # If the end token is reached, select the sequence as the output\n# #                 word_t = ''.join([reverse_target_char_index[token.item()] for token in seq[1:-1]])\n# #                 return word_t\n\n# #     # If the maximum sequence length is reached, select the best sequence in the beam as the output\n# #     best_sequence = beam[0][0]\n# #     word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n\n# #     return word_t\nimport heapq\n\ndef beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,\n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n                num_encoder_tokens, num_decoder_tokens, beam_width, device):\n\n    word_t = ''\n\n    # Encode the input word\n    data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n    for t, char in enumerate(word):\n        data[t, 0] = input_char_index[char]\n\n    data[t + 1:, 0] = input_char_index[\" \"]\n\n    data = torch.tensor(data, dtype=torch.int64).to(device)\n\n    with torch.no_grad():\n        hidden,cell = model.encoder(data)\n\n    # Initialize beam\n    initial_sequence = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n\n    for _ in range(max_decoder_seq_length):\n        candidates = []\n        for score, seq, hidden in beam:\n            last_token = seq[-1].item()\n            if last_token == output_char_index['\\n']:\n                # If the sequence ends with the end token, add it to the candidates\n                candidates.append((score, seq, hidden))\n                continue\n\n            x = torch.tensor(np.array(last_token).reshape(1,)).to(device)\n            output, hidden,cell = model.decoder(x, hidden.squeeze(0),cell)\n            probabilities = F.softmax(output, dim=1)\n\n            # Get the top-k probabilities and tokens\n            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n\n            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n                new_hidden = hidden.clone().unsqueeze(0)\n                candidates.append((score + torch.log(prob).item(), new_seq, new_hidden))\n\n        # Select top-k candidates based on the accumulated scores\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n    # Select the best sequence from the beam as the output\n    best_score, best_sequence, _ = beam[0]\n    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n\n    return word_t","metadata":{"execution":{"iopub.status.busy":"2023-05-12T10:22:41.539935Z","iopub.execute_input":"2023-05-12T10:22:41.540523Z","iopub.status.idle":"2023-05-12T10:22:41.576974Z","shell.execute_reply.started":"2023-05-12T10:22:41.540473Z","shell.execute_reply":"2023-05-12T10:22:41.575573Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"total_words = len(val_X)\ncorrect_pred = 0\nfor i in range(total_words):\n  #  print(val_Y[i][1:-1])\n    decoded_sentence = beam_search(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens,4,device)\n#    print(translate(model, train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n             # reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n             # num_encoder_tokens, num_decoder_tokens, device))\n    #print(decoded_sentence)\n    #print('\\n')\n    if val_Y[i][1:-1]== decoded_sentence:\n        correct_pred += 1\n       # print('True')\ntest_accuracy = correct_pred / total_words\n\nprint(test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T10:31:24.543489Z","iopub.execute_input":"2023-05-12T10:31:24.544214Z","iopub.status.idle":"2023-05-12T10:32:46.303354Z","shell.execute_reply.started":"2023-05-12T10:31:24.544177Z","shell.execute_reply":"2023-05-12T10:32:46.302220Z"},"trusted":true},"execution_count":242,"outputs":[{"name":"stdout","text":"0.004638671875\n","output_type":"stream"}]},{"cell_type":"code","source":"import heapq\n\ndef beam_search(model, word, input_char_index, output_char_index, reverse_input_char_index,\n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n\n    word_t = ''\n\n    # Encode the input word\n    data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n    for t, char in enumerate(word):\n        data[t, 0] = input_char_index[char]\n\n    data[t + 1:, 0] = input_char_index[\" \"]\n\n    data = torch.tensor(data, dtype=torch.int64).to(device)\n\n    with torch.no_grad():\n        hidden,cell = model.encoder(data)\n\n    # Initialize beam\n    initial_sequence = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n\n    for _ in range(max_decoder_seq_length):\n        candidates = []\n        for score, seq, hidden in beam:\n            last_token = seq[-1].item()\n            if last_token == output_char_index['\\n']:\n                # If the sequence ends with the end token, add it to the candidates\n                candidates.append((score, seq, hidden))\n                continue\n\n            x = torch.tensor(np.array(last_token).reshape(1,)).to(device)\n            output, hidden,cell = model.decoder(x, hidden.squeeze(0),cell)\n            probabilities = F.softmax(output, dim=1)\n\n            # Get the top-k probabilities and tokens\n            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n\n            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n                new_hidden = hidden.clone().unsqueeze(0)\n                length_penalty_factor = ((len(new_seq) - 1) / 5) ** length_penalty  # Adjust penalty factor as needed\n                candidates.append((score + torch.log(prob).item() / length_penalty_factor, new_seq, new_hidden))\n\n        # Select top-k candidates based on the accumulated scores\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n    # Select the best sequence from the beam as the output\n    best_score, best_sequence, _ = max(beam, key=lambda x: x[0])\n    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n\n    return word_t","metadata":{"execution":{"iopub.status.busy":"2023-05-12T10:39:14.461308Z","iopub.execute_input":"2023-05-12T10:39:14.461721Z","iopub.status.idle":"2023-05-12T10:39:14.479187Z","shell.execute_reply.started":"2023-05-12T10:39:14.461687Z","shell.execute_reply":"2023-05-12T10:39:14.478169Z"},"trusted":true},"execution_count":250,"outputs":[]},{"cell_type":"code","source":"total_words = len(val_X)\ncorrect_pred = 0\nfor i in range(total_words):\n  #  print(val_Y[i][1:-1])\n    decoded_sentence = beam_search(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens,1,device)\n#    print(translate(model, train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n             # reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n             # num_encoder_tokens, num_decoder_tokens, device))\n    #print(decoded_sentence)\n    #print('\\n')\n    if val_Y[i][1:-1]== decoded_sentence:\n        correct_pred += 1\n       # print('True')\ntest_accuracy = correct_pred / total_words\n\nprint(test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T10:42:06.554677Z","iopub.execute_input":"2023-05-12T10:42:06.555077Z","iopub.status.idle":"2023-05-12T10:42:37.825175Z","shell.execute_reply.started":"2023-05-12T10:42:06.555044Z","shell.execute_reply":"2023-05-12T10:42:37.824204Z"},"trusted":true},"execution_count":259,"outputs":[{"name":"stdout","text":"0.26953125\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For GRU Code start from here","metadata":{"execution":{"iopub.status.busy":"2023-05-12T10:43:29.978706Z","iopub.execute_input":"2023-05-12T10:43:29.979648Z","iopub.status.idle":"2023-05-12T10:43:29.985783Z","shell.execute_reply.started":"2023-05-12T10:43:29.979609Z","shell.execute_reply":"2023-05-12T10:43:29.984635Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"code","source":"#For GRU\nclass EncoderGRU(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(EncoderGRU, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (seq_length, N, embedding_size)\n        outputs, hidden = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n        return hidden\n\nclass DecoderGRU(nn.Module):\n    def __init__(\n        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n        super(DecoderGRU, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, output_size)\n       # self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x, hidden):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (1, N, embedding_size)\n\n        outputs, hidden = self.rnn(embedding, hidden)\n        # outputs shape: (1, N, hidden_size)\n\n        predictions = self.fc(outputs)\n        # predictions shape: (1, N, length_target_vocabulary) to send it to\n        # loss function we want it to be (N, length_target_vocabulary) so we're\n        # just gonna remove the first dim\n        # predictions = self.softmax(predictions)\n        predictions = predictions.squeeze(0)\n        return predictions, hidden\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:47:45.896559Z","iopub.execute_input":"2023-05-12T11:47:45.897718Z","iopub.status.idle":"2023-05-12T11:47:45.916897Z","shell.execute_reply.started":"2023-05-12T11:47:45.897661Z","shell.execute_reply":"2023-05-12T11:47:45.915862Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.2):\n        batch_size = source.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = num_decoder_tokens\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n        hidden= self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n        x = target[0]\n\n        for t in range(1, target_len):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden = self.decoder(x, hidden)\n\n            # Store next output prediction\n            outputs[t] = output\n\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else best_guess\n\n        return outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:47:51.076735Z","iopub.execute_input":"2023-05-12T11:47:51.077088Z","iopub.status.idle":"2023-05-12T11:47:51.085064Z","shell.execute_reply.started":"2023-05-12T11:47:51.077057Z","shell.execute_reply":"2023-05-12T11:47:51.084011Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.001\nbatch_size = 32\nload_model = False\ninput_size_encoder = num_encoder_tokens\ninput_size_decoder = num_decoder_tokens\noutput_size = num_decoder_tokens\nencoder_embedding_size = 256\ndecoder_embedding_size = 256\nhidden_size = 256  # Needs to be the same for both RNN's\nnum_enc_layers = 1\nnum_dec_layers = 1\nenc_dropout = 0.1\ndec_dropout = 0.1\ntraining = False","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:51:46.888022Z","iopub.execute_input":"2023-05-12T11:51:46.888387Z","iopub.status.idle":"2023-05-12T11:51:46.894685Z","shell.execute_reply.started":"2023-05-12T11:51:46.888357Z","shell.execute_reply":"2023-05-12T11:51:46.893709Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"#EGRU RUN Only\nencoder_net = EncoderGRU(\n    input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout\n).to(device)\n\ndecoder_net = DecoderGRU(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    num_dec_layers,\n    dec_dropout\n).to(device)\n\nmodel = Seq2Seq(encoder_net, decoder_net).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:51:47.101974Z","iopub.execute_input":"2023-05-12T11:51:47.104733Z","iopub.status.idle":"2023-05-12T11:51:47.130482Z","shell.execute_reply.started":"2023-05-12T11:51:47.104691Z","shell.execute_reply":"2023-05-12T11:51:47.129674Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_ds_x = torch.split(input_data,batch_size,dim=1)\ntrain_ds_y = torch.split(target_data,batch_size,dim=1)\ninput_data_val = input_data_val.to(device)\ntarget_data_val = target_data_val.to(device)\ntarget_val = target_data_val[1:].reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T11:51:47.309449Z","iopub.execute_input":"2023-05-12T11:51:47.309818Z","iopub.status.idle":"2023-05-12T11:51:47.326027Z","shell.execute_reply.started":"2023-05-12T11:51:47.309790Z","shell.execute_reply":"2023-05-12T11:51:47.325089Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# this cell is only for training, not to be used now as we have saved the model\n\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    model.eval()\n    model.train()\n\n    for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n        # Get input and targets and get to cuda\n        inp_data = x.to(device)\n        target = y.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n        # way that we have output_words * batch_size that we want to send in into\n        # our cost function, so we need to do some reshapin. While we're at it\n        # Let's also remove the start token while we're at it\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n        \n        \n#torch.save(model.state_dict(),'models\\model_pytorch_noAT_state.pt')\n#torch.save(model,'models\\model_pytorch_noAT.pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:01:52.627088Z","iopub.execute_input":"2023-05-12T12:01:52.628074Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[Epoch 0 / 10]\n[Epoch 1 / 10]\n[Epoch 2 / 10]\n[Epoch 3 / 10]\n[Epoch 4 / 10]\n[Epoch 5 / 10]\n","output_type":"stream"}]},{"cell_type":"code","source":"def translate(model, word, input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device):\n    \n    word_t = ''\n    data = np.zeros((max_encoder_seq_length,1), dtype=\"int64\")\n    for t, char in enumerate(word):\n        data[t, 0] = input_char_index[char]\n        \n    data[t + 1 :,0] = input_char_index[\" \"]\n    \n    data = torch.tensor(data,dtype=torch.int64).to(device)\n\n    with torch.no_grad():\n        hidden = model.encoder(data)\n        \n    x = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n    \n    for t in range(1, max_decoder_seq_length):\n         output, hidden = model.decoder(x, hidden)\n       #  print(output.shape)\n         best_guess = output.argmax(1)\n         x = best_guess\n         ch = reverse_target_char_index[x.item()]\n         if ch == '\\n':\n           break\n         else:\n            word_t = word_t+ch\n\n    return word_t","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words = len(val_X)\ncorrect_pred = 0\nfor i in range(total_words):\n    #print(train_Y[i])\n    decoded_sentence = translate(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n              num_encoder_tokens, num_decoder_tokens, device)\n#    print(translate(model, train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n             # reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n             # num_encoder_tokens, num_decoder_tokens, device))\n    #print(decoded_sentence)\n    if val_Y[i][1:-1]== decoded_sentence:\n        correct_pred += 1\n       # print('True')\ntest_accuracy = correct_pred / total_words\n\nprint(test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}