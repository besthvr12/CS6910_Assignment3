{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport csv\nimport matplotlib as mlp\nimport torch.nn as nn\nimport numpy as np\nimport torch.optim as optim\nimport os\nimport random\nimport torch\nimport PIL\nimport pandas as pd\nimport cv2\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\nimport pathlib\nimport torch.nn.functional as F\nimport glob\nimport matplotlib.pyplot as plt\nimport shutil\nimport matplotlib.ticker as ticker\nimport heapq\nfrom collections import defaultdict\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-21T09:11:56.234376Z","iopub.execute_input":"2023-05-21T09:11:56.235068Z","iopub.status.idle":"2023-05-21T09:12:00.517572Z","shell.execute_reply.started":"2023-05-21T09:11:56.235037Z","shell.execute_reply":"2023-05-21T09:12:00.516463Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = []\nY_train = []\nX_val = []\nY_val = []\nX_test = []\nY_test = []","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:00.525105Z","iopub.execute_input":"2023-05-21T09:12:00.525499Z","iopub.status.idle":"2023-05-21T09:12:00.532229Z","shell.execute_reply.started":"2023-05-21T09:12:00.525464Z","shell.execute_reply":"2023-05-21T09:12:00.530334Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_train.csv\")\nread_tsv = csv.reader(tsv_file)\n\npad1 = \"\\t\"\nspace1 = \"\\n\"\nfor i in read_tsv:   \n    Y_train.append(i[1])\n    X_train.append(i[0])\n\nX_train = np.array(X_train)\n\nval_tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_valid.csv\")\nval_read_tsv = csv.reader(val_tsv_file)\npad2 = \"\\t\"\nspace2 = \"\\n\"\nfor i in val_read_tsv:\n    Y_val.append(i[1])\n    X_val.append(i[0])\nX_val= np.array(X_val)\n\ntest_tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_test.csv\")\ntest_read_tsv = csv.reader(test_tsv_file)\npad3 = \"\\t\"\nspace3 = \"\\n\"\nfor i in test_read_tsv:\n    Y_test.append(i[1])\n    X_test.append(i[0])\n\nX_test = np.array(X_test)\ntrainsize=len(X_train)\ntestsize = len(X_test)\nvalidationsize = len(X_val)\n\nprint(\"Number of Training samples:\", trainsize)\nprint(\"Number of Test Samples:\" , testsize)\nprint(\"Number of Validation Samples:\", validationsize)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:00.533407Z","iopub.execute_input":"2023-05-21T09:12:00.534349Z","iopub.status.idle":"2023-05-21T09:12:00.650410Z","shell.execute_reply.started":"2023-05-21T09:12:00.534322Z","shell.execute_reply":"2023-05-21T09:12:00.649234Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of Training samples: 51200\nNumber of Test Samples: 4096\nNumber of Validation Samples: 4096\n","output_type":"stream"}]},{"cell_type":"code","source":"Y_train = np.array(Y_train)\nfor i in range(Y_train.shape[0]):\n    Y_train[i] = pad1 + Y_train[i] + space1\n\nY_val = np.array(Y_val)\nfor i in range(Y_val.shape[0]):\n    Y_val[i] = pad2 + Y_val[i] + space2\n    \nY_test = np.array(Y_test)\nfor i in range(Y_test.shape[0]):\n    Y_test[i] = pad3+ Y_test[i] + space3\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:00.653731Z","iopub.execute_input":"2023-05-21T09:12:00.654049Z","iopub.status.idle":"2023-05-21T09:12:00.752022Z","shell.execute_reply.started":"2023-05-21T09:12:00.654025Z","shell.execute_reply":"2023-05-21T09:12:00.751184Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"traintargetsize=len(Y_train)\ntesttargetsize = len(Y_test)\ntesttargetsize = len(Y_val)\nprint(\"TrainSize of Target \",traintargetsize)\nprint(\"TrainSize of Target \",testtargetsize)\nprint(\"TrainSize of Target \",testtargetsize)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:00.753330Z","iopub.execute_input":"2023-05-21T09:12:00.753644Z","iopub.status.idle":"2023-05-21T09:12:00.759693Z","shell.execute_reply.started":"2023-05-21T09:12:00.753613Z","shell.execute_reply":"2023-05-21T09:12:00.758491Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"TrainSize of Target  51200\nTrainSize of Target  4096\nTrainSize of Target  4096\n","output_type":"stream"}]},{"cell_type":"code","source":"input_corpus = set()\noutput_corpus = set()\ninput_corpus = set(char for word in X_train for char in word if char not in input_corpus)\noutput_corpus = set(char for word in Y_train for char in word if char not in output_corpus)\ninput_corpus.add(\" \")\noutput_corpus.add(\" \")\ninput_corpus = sorted(list(input_corpus))\noutput_corpus = sorted(list(output_corpus))\nval_input_corpus = set()\nval_output_corpus = set()\nmax_encoder_seq_length = max([len(txt) for txt in X_train]) + 2\nmax_decoder_seq_length = max([len(txt) for txt in Y_train])\n\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)\nval_input_corpus = set(char for word in X_val for char in word if char not in val_input_corpus)\nval_output_corpus = set(char for word in Y_val for char in word if char not in val_output_corpus)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:00.761379Z","iopub.execute_input":"2023-05-21T09:12:00.761998Z","iopub.status.idle":"2023-05-21T09:12:01.042151Z","shell.execute_reply.started":"2023-05-21T09:12:00.761967Z","shell.execute_reply":"2023-05-21T09:12:01.040223Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Max sequence length for inputs: 30\nMax sequence length for outputs: 21\n","output_type":"stream"}]},{"cell_type":"code","source":"num_encoder_tokens = len(input_corpus)\nnum_decoder_tokens = len(output_corpus)\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:01.043865Z","iopub.execute_input":"2023-05-21T09:12:01.044570Z","iopub.status.idle":"2023-05-21T09:12:01.050931Z","shell.execute_reply.started":"2023-05-21T09:12:01.044535Z","shell.execute_reply":"2023-05-21T09:12:01.049738Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of unique input tokens: 27\nNumber of unique output tokens: 65\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\ninput_data = np.zeros((max_encoder_seq_length,len(X_train)), dtype=\"int64\")\ntarget_data = np.zeros((max_decoder_seq_length,len(X_train)), dtype=\"int64\")\ninput_data_val = np.zeros((max_encoder_seq_length,len(X_val)), dtype=\"int64\")\ntarget_data_val = np.zeros((max_decoder_seq_length,len(X_val)), dtype=\"int64\")","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:01.052351Z","iopub.execute_input":"2023-05-21T09:12:01.053527Z","iopub.status.idle":"2023-05-21T09:12:01.062908Z","shell.execute_reply.started":"2023-05-21T09:12:01.053491Z","shell.execute_reply":"2023-05-21T09:12:01.061938Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"input_size_encoder = num_encoder_tokens\ninput_size_decoder = num_decoder_tokens\noutput_size = num_decoder_tokens\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:01.064468Z","iopub.execute_input":"2023-05-21T09:12:01.064848Z","iopub.status.idle":"2023-05-21T09:12:01.071888Z","shell.execute_reply.started":"2023-05-21T09:12:01.064813Z","shell.execute_reply":"2023-05-21T09:12:01.070987Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def create_char_index(corpus):\n    char_index = defaultdict(int)\n    for i, char in enumerate(corpus):\n        if char not in char_index:\n            char_index[char] = i\n    return dict(char_index)\n\ninput_char_index = create_char_index(input_corpus)\noutput_char_index = create_char_index(output_corpus)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:01.073319Z","iopub.execute_input":"2023-05-21T09:12:01.074070Z","iopub.status.idle":"2023-05-21T09:12:01.084074Z","shell.execute_reply.started":"2023-05-21T09:12:01.074038Z","shell.execute_reply":"2023-05-21T09:12:01.083218Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for i in range(len(X_train)):\n    x = X_train[i]\n    y = Y_train[i]\n    count=0\n    size = x.size\n    for t, char in enumerate(x):\n        input_data[t, i] = input_char_index[char]\n    count=count+1 \n    input_data[t + 1 :,i] = input_char_index[\" \"]\n    count1=1\n    for t, char in enumerate(y):\n        target_data[t, i] = output_char_index[char]\n        count1=count+1     \n    target_data[t + 1 :,i] = output_char_index[\" \"]\n    \n\ndata_type = torch.int64\ndata_type1=torch.float64\nfor i in range(len(X_val)):\n    x=X_val[i]\n    y=Y_val[i]\n    count = 0\n    size - x.size\n    for t, char in enumerate(x):\n        input_data_val[t, i] = input_char_index[char]\n        count = count +1\n    input_data_val[t + 1 :,i] = input_char_index[\" \"]\n    \n    count = 0\n    for t, char in enumerate(y):\n        target_data_val[t, i] = output_char_index[char]\n        count = count +1\n            \n    target_data_val[t + 1 :,i] = output_char_index[\" \"]\n\ninput_data_val = torch.tensor(input_data_val,dtype=data_type)\ntarget_data_val = torch.tensor(target_data_val,dtype=data_type)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:01.085733Z","iopub.execute_input":"2023-05-21T09:12:01.086151Z","iopub.status.idle":"2023-05-21T09:12:02.167108Z","shell.execute_reply.started":"2023-05-21T09:12:01.086095Z","shell.execute_reply":"2023-05-21T09:12:02.166100Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\ninput_data = torch.tensor(input_data,dtype=data_type)\nreverse_input_char_index = {i: char for char, i in input_char_index.items()}\nreverse_target_char_index = {i: char for char, i in output_char_index.items()}\ntarget_data = torch.tensor(target_data,dtype=data_type)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.168767Z","iopub.execute_input":"2023-05-21T09:12:02.169165Z","iopub.status.idle":"2023-05-21T09:12:02.191009Z","shell.execute_reply.started":"2023-05-21T09:12:02.169133Z","shell.execute_reply":"2023-05-21T09:12:02.190132Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout,opsize):\n        super(Encoder, self).__init__()\n        self.dropout, self.hidden_size, self.num_layers,self.identity_init,self.init_h0 = nn.Dropout(dropout) , hidden_size , num_layers,torch.eye(hidden_size),nn.Linear(num_layers * 2, num_layers)\n        self.embedding,self.rnn = nn.Embedding(input_size, embedding_size) , nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True) \n        hid_2 =hidden_size*2\n        self.fc_hidden, self.fc_cell = nn.Linear(hid_2, hidden_size)  , nn.Linear(hid_2, hidden_size)\n    \n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n        a = self.embedding(x)\n        embedding = self.dropout(a)\n        # embedding shape: (seq_length, N, embedding_size)\n        dim_value = 2\n        encoder_states, (hidden, cell) = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n        hid1 = hidden[0:1]\n        hid2 = hidden[1:2]\n        hid3 = torch.cat(( hid1, hid2), dim=dim_value)\n        hidden = self.fc_hidden(hid3)\n        \n        cel1 = cell[0:1]\n        cel2 = cell[1:2]\n        cel3 = torch.cat(( cel1,  cel2), dim=dim_value)\n        cell = self.fc_cell(cel3)\n\n        return encoder_states, hidden, cell","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.195468Z","iopub.execute_input":"2023-05-21T09:12:02.195756Z","iopub.status.idle":"2023-05-21T09:12:02.206327Z","shell.execute_reply.started":"2023-05-21T09:12:02.195731Z","shell.execute_reply":"2023-05-21T09:12:02.205041Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(Decoder, self).__init__()\n        self.dropout,self.hidden_size,self.num_layers,self.embedding = nn.Dropout(dropout),hidden_size,num_layers,nn.Embedding(input_size, embedding_size)\n        val,zero = 1,0\n        hidsize =  hidden_size * 2\n        hidemb = hidsize + embedding_size\n        self.rnn = nn.LSTM(hidemb, hidden_size, num_layers)       \n        hid3 = hidden_size * 3 \n        self.energy ,self.fc, self.softmax,self.relu  = nn.Linear(hid3, val),nn.Linear(hidden_size, output_size),nn.Softmax(dim=zero),nn.Hardshrink()\n\n    def forward(self, x, encoder_states, hidden, cell):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n        a= self.embedding(x)\n        hid = hidden.unsqueeze(0)\n        embedding = self.dropout(a)\n        length_seq = encoder_states.shape[0]\n        # embedding shape: (1, N, embedding_size)\n        val  = 2\n        sequence_length = encoder_states.shape[0]\n        h_reshaped = hidden.repeat(length_seq , 1, 1)\n\n        concat = torch.cat((h_reshaped, encoder_states), dim=val)\n\n        \n        attention = (self.softmax(self.relu(self.energy(concat)))).permute(1,2,0)\n\n        \n\n        encoder_states = self.relu(encoder_states.permute(1,0,2))\n\n        \n        context_vector = torch.bmm(attention, encoder_states)\n        context_vector = context_vector.permute(1,0,2)\n        \n        rnn_concat = torch.cat((context_vector,embedding),dim=val)\n        rnn_input = rnn_concat\n       # rnn_input = self.relu(rnn_input)\n        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n        # outputs shape: (1, N, hidden_size)\n\n        predictions = self.fc(outputs)\n        results = self.softmax(predictions)\n\n        # predictions shape: (1, N, length_target_vocabulary) to send it to\n        # loss function we want it to be (N, length_target_vocabulary) so we're\n        # just gonna remove the first dim\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell, attention,results","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.207969Z","iopub.execute_input":"2023-05-21T09:12:02.208328Z","iopub.status.idle":"2023-05-21T09:12:02.222535Z","shell.execute_reply.started":"2023-05-21T09:12:02.208296Z","shell.execute_reply":"2023-05-21T09:12:02.221855Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        x = target[0]\n        variables = 0\n        target_vocab_size = num_decoder_tokens\n        input_vocab_size = num_encoder_tokens\n        outputs = torch.zeros(target.shape[0], source.shape[1], target_vocab_size).to(device)\n\n        encoder_states, hidden, cell = self.encoder(source)\n        # Grab the first input to the Decoder which will be <SOS> token\n\n        for t in range(1, target.shape[0]):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden, cell, _,results = self.decoder(x, encoder_states, hidden, cell)\n\n            # Store next output prediction\n            outputs[t] = output\n            res = output[0:]\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n            tf = teacher_force_ratio\n            ran = random.random()\n            if ran <  tf:\n                x = target[t]\n            else:\n                x = best_guess\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.223848Z","iopub.execute_input":"2023-05-21T09:12:02.224491Z","iopub.status.idle":"2023-05-21T09:12:02.239531Z","shell.execute_reply.started":"2023-05-21T09:12:02.224456Z","shell.execute_reply":"2023-05-21T09:12:02.238690Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#For RNN\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout,opsize):\n        super(EncoderRNN, self).__init__()\n        self.dropout, self.hidden_size,self.num_layers = nn.Dropout(dropout),hidden_size,num_layers\n        bidirection = True\n        self.embedding, self.rnn = nn.Embedding(input_size, embedding_size),nn.RNN(embedding_size, hidden_size, num_layers, bidirectional=True)\n        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, bidirectional=bidirection )\n        hid_2 = hidden_size * 2\n        self.fc_hidden , self.fc_cell = nn.Linear(hid_2, hidden_size) , nn.Linear(hid_2, hidden_size)\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n        a = self.embedding(x)\n        embedding = self.dropout(a)\n        # embedding shape: (seq_length, N, embedding_size)\n        dim_value = 2\n        encoder_states, hidden = self.rnn(embedding)\n\n        hid1 = hidden[0:1]\n        hid2 = hidden[1:2]\n        hid3 = torch.cat((hidden[0:1], hidden[1:2]), dim=dim_value)\n        hidden = self.fc_hidden(hid3)\n\n\n        return encoder_states, hidden\nclass DecoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(DecoderRNN, self).__init__()\n        self.dropout,self.hidden_size,self.num_layers ,self.embedding = nn.Dropout(dropout),hidden_size,num_layers, nn.Embedding(input_size, embedding_size)\n\n        val,zero = 1,0\n        hidsize =  hidden_size * 2\n        hidemb = hidsize + embedding_size\n        self.rnn = nn.RNN(hidemb , hidden_size, num_layers)\n        hid_3 = hidden_size * 3\n        self.energy, self.fc,self.softmax,self.relu = nn.Linear(hid_3, val),nn.Linear(hidden_size, output_size),nn.Softmax(dim=zero),nn.Hardshrink()\n\n\n    def forward(self, x, encoder_states, hidden):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n        a= self.embedding(x)\n        hid = hidden.unsqueeze(0)\n        embedding = self.dropout(a)\n        length_seq = encoder_states.shape[0]\n        # embedding shape: (1, N, embedding_size)\n        val  = 2\n        sequence_length = encoder_states.shape[0]\n        h_reshaped = hidden.repeat(length_seq , 1, 1)\n        # h_reshaped: (seq_length, N, hidden_size*2)\n        concat = torch.cat((h_reshaped, encoder_states), dim=val)\n\n        \n        attention = (self.softmax(self.relu(self.energy(concat)))).permute(1,2,0)\n        encoder_states = self.relu(encoder_states.permute(1,0,2))\n\n        \n        context_vector = torch.bmm(attention, encoder_states)\n        context_vector = context_vector.permute(1,0,2)\n        \n        rnn_concat = torch.cat((context_vector,embedding),dim=val)\n        rnn_input = rnn_concat\n           # rnn_input = self.relu(rnn_input)\n        outputs, (hidden) = self.rnn(rnn_input, (hidden))\n            # outputs shape: (1, N, hidden_size)\n\n        predictions = self.fc(outputs)\n        results = self.softmax(predictions)\n\n        predictions = predictions.squeeze(0)\n\n        return predictions,hidden,attention,results                                       \nclass Seq2SeqGR(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2SeqGR, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        x = target[0]\n        variables = 0\n        target_vocab_size = num_decoder_tokens\n        input_vocab_size = num_encoder_tokens\n        outputs = torch.zeros(target.shape[0], source.shape[1], target_vocab_size).to(device)\n\n        encoder_states, hidden = self.encoder(source)\n        # Grab the first input to the Decoder which will be <SOS> token\n\n        for t in range(1, target.shape[0]):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden, _, results = self.decoder(x, encoder_states, hidden)\n\n            # Store next output prediction\n            outputs[t] = output\n            res = output[0:]\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n            tf = teacher_force_ratio\n            ran = random.random()\n            if ran <  tf:\n                x = target[t]\n            else:\n                x = best_guess\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.241054Z","iopub.execute_input":"2023-05-21T09:12:02.241436Z","iopub.status.idle":"2023-05-21T09:12:02.266275Z","shell.execute_reply.started":"2023-05-21T09:12:02.241404Z","shell.execute_reply":"2023-05-21T09:12:02.265215Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#For GRU\nclass EncoderGRU(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout,opsize):\n        super(EncoderGRU, self).__init__()\n        self.dropout, self.hidden_size,self.num_layers = nn.Dropout(dropout),hidden_size,num_layers\n        bidirection = True\n        self.embedding, self.rnn = nn.Embedding(input_size, embedding_size),nn.RNN(embedding_size, hidden_size, num_layers, bidirectional=True)\n        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirection )\n        hid_2 = hidden_size * 2\n        self.fc_hidden , self.fc_cell = nn.Linear(hid_2, hidden_size) , nn.Linear(hid_2, hidden_size)\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n\n        a = self.embedding(x)\n        embedding = self.dropout(a)\n        # embedding shape: (seq_length, N, embedding_size)\n        dim_value = 2\n        encoder_states, hidden = self.rnn(embedding)\n\n        hid1 = hidden[0:1]\n        hid2 = hidden[1:2]\n        hid3 = torch.cat((hidden[0:1], hidden[1:2]), dim=dim_value)\n        hidden = self.fc_hidden(hid3)\n\n\n        return encoder_states, hidden\nclass DecoderGRU(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(DecoderGRU, self).__init__()\n        self.dropout,self.hidden_size,self.num_layers ,self.embedding = nn.Dropout(dropout),hidden_size,num_layers, nn.Embedding(input_size, embedding_size)\n\n        val,zero = 1,0\n        hidsize =  hidden_size * 2\n        hidemb = hidsize + embedding_size\n        self.rnn = nn.GRU  (hidemb , hidden_size, num_layers)  \n        hid_3 = hidden_size * 3\n        self.energy, self.fc,self.softmax,self.relu = nn.Linear(hid_3, val),nn.Linear(hidden_size, output_size),nn.Softmax(dim=zero),nn.Hardshrink()\n\n\n    def forward(self, x, encoder_states, hidden):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n        a= self.embedding(x)\n        hid = hidden.unsqueeze(0)\n        embedding = self.dropout(a)\n        length_seq = encoder_states.shape[0]\n        # embedding shape: (1, N, embedding_size)\n        val  = 2\n        sequence_length = encoder_states.shape[0]\n        h_reshaped = hidden.repeat(length_seq , 1, 1)\n        # h_reshaped: (seq_length, N, hidden_size*2)\n        concat = torch.cat((h_reshaped, encoder_states), dim=val)\n\n        \n        attention = (self.softmax(self.relu(self.energy(concat)))).permute(1,2,0)\n\n        \n\n        encoder_states = self.relu(encoder_states.permute(1,0,2))\n\n        \n        context_vector = torch.bmm(attention, encoder_states)\n        context_vector = context_vector.permute(1,0,2)\n        \n        rnn_concat = torch.cat((context_vector,embedding),dim=val)\n        rnn_input = rnn_concat\n\n        outputs, (hidden) = self.rnn(rnn_input, (hidden))\n   \n\n        predictions = self.fc(outputs)\n        results = self.softmax(predictions)\n\n        predictions = predictions.squeeze(0)\n\n        return predictions,hidden,attention,results  \n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.267543Z","iopub.execute_input":"2023-05-21T09:12:02.267919Z","iopub.status.idle":"2023-05-21T09:12:02.287121Z","shell.execute_reply.started":"2023-05-21T09:12:02.267888Z","shell.execute_reply":"2023-05-21T09:12:02.286234Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def beam_find(model, word, norm,input_char_index, output_char_index, reverse_input_char_index,\n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n\n    word_t = ''\n\n    word_val = 0\n    attention = []\n    data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n    datatype = torch.int64\n    word_size1 =len(word)\n    for t in range(word_size1):\n        char = word[t]\n        data[t, word_val] = input_char_index[char]\n    word_size =len(word)\n    data[ word_size:, word_val] = input_char_index[\" \"]\n    data = torch.tensor(data, dtype=datatype).to(device)\n\n    with torch.no_grad():\n        encoder_states,hidden,cell= model.encoder(data)\n\n    # Initialize beam\n    \n    value = 1\n    ovalue=-1\n    padding = '\\t'\n    initial_sequence = torch.tensor(np.array(output_char_index[padding]).reshape(value,)).to(device)\n    zero = 0\n    res =hidden.unsqueeze(0)\n    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n    start ='\\n'\n    for _ in range(max_decoder_seq_length):\n        candidates = []\n        options=[]\n        for score, seq, hidden in beam:\n            count = 0\n            last_token = seq[ovalue].item()\n            if last_token == output_char_index[start]:\n                # If the sequence ends with the end token, add it to the candidates\n                candidates.append((score, seq, hidden))\n                count =count+1\n                continue\n            tempval = np.array(last_token).reshape(value,)\n            x = torch.tensor(tempval).to(device)\n            output, hidden,cell,at,res = model.decoder(x,encoder_states ,hidden.squeeze(0),cell)\n            attention.append(at.detach().cpu().numpy())\n            res = output[0:]\n            probabilities = F.softmax(output, dim=1)\n            result =probabilities\n            # Get the top-k probabilities and tokens\n            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n            tops = topk_probs[0]\n            topk = topk_tokens[0]\n            for prob, token in zip(tops, topk):\n                new_s= token.unsqueeze(0)\n                new_seq = torch.cat((seq, new_s), dim=0)\n                new_hidden = hidden.clone().unsqueeze(0)\n                penalty = len(new_seq) - 1\n                len_pen = length_penalty\n                length_penalty_factor = ((penalty) / 5) **  len_pen   # Adjust penalty factor as needed\n                candidates.append((score + torch.log(prob).item() / length_penalty_factor, new_seq, new_hidden))\n                options.append((score + torch.log(prob).item() / length_penalty_factor))\n        # Select top-k candidates based on the accumulated scores\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n    # Select the best sequence from the beam as the output\n    best_res = \"\"\n    best_score, best_sequence, _ = max(beam, key=lambda x: x[zero])\n    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[value:ovalue]])\n    best_res = word_t\n\n    return word_t,attention\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.288599Z","iopub.execute_input":"2023-05-21T09:12:02.288935Z","iopub.status.idle":"2023-05-21T09:12:02.307315Z","shell.execute_reply.started":"2023-05-21T09:12:02.288904Z","shell.execute_reply":"2023-05-21T09:12:02.306368Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n\nimport heapq\n#     return word_t\ndef beam_findGR(model, word, norm,input_char_index, output_char_index, reverse_input_char_index,\n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n\n    word_t = ''\n\n    word_val = 0\n    attention = []\n    data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n    datatype = torch.int64\n    word_size1 =len(word)\n    for t in range(word_size1):\n        char = word[t]\n        data[t, word_val] = input_char_index[char]\n    word_size =len(word)\n    data[ word_size:, word_val] = input_char_index[\" \"]\n    data = torch.tensor(data, dtype=datatype).to(device)\n\n    with torch.no_grad():\n        encoder_states,hidden = model.encoder(data)\n\n    # Initialize beam\n    \n    value = 1\n    ovalue=-1\n    padding = '\\t'\n    initial_sequence = torch.tensor(np.array(output_char_index[padding]).reshape(value,)).to(device)\n    zero = 0\n    res =hidden.unsqueeze(0)\n    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n    start ='\\n'\n    for _ in range(max_decoder_seq_length):\n        candidates = []\n        options=[]\n        for score, seq, hidden in beam:\n            count = 0\n            last_token = seq[ovalue].item()\n            if last_token == output_char_index[start]:\n                # If the sequence ends with the end token, add it to the candidates\n                candidates.append((score, seq, hidden))\n                count =count+1\n                continue\n            tempval = np.array(last_token).reshape(value,)\n            x = torch.tensor(tempval).to(device)\n            output, hidden,at,res = model.decoder(x,encoder_states ,hidden.squeeze(0))\n            attention.append(at.detach().cpu().numpy())\n            res = output[0:]\n            probabilities = F.softmax(output, dim=1)\n            result =probabilities\n            # Get the top-k probabilities and tokens\n            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n            tops = topk_probs[0]\n            topk = topk_tokens[0]\n            for prob, token in zip(tops, topk):\n                new_s= token.unsqueeze(0)\n                new_seq = torch.cat((seq, new_s), dim=0)\n                new_hidden = hidden.clone().unsqueeze(0)\n                penalty = len(new_seq) - 1\n                len_pen = length_penalty\n                length_penalty_factor = ((penalty) / 5) **  len_pen   # Adjust penalty factor as needed\n                candidates.append((score + torch.log(prob).item() / length_penalty_factor, new_seq, new_hidden))\n                options.append((score + torch.log(prob).item() / length_penalty_factor))\n        # Select top-k candidates based on the accumulated scores\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n    # Select the best sequence from the beam as the output\n    best_res = \"\"\n    best_score, best_sequence, _ = max(beam, key=lambda x: x[zero])\n    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[value:ovalue]])\n    best_res = word_t\n\n    return word_t,attention\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.310528Z","iopub.execute_input":"2023-05-21T09:12:02.310815Z","iopub.status.idle":"2023-05-21T09:12:02.329147Z","shell.execute_reply.started":"2023-05-21T09:12:02.310792Z","shell.execute_reply":"2023-05-21T09:12:02.328267Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training(num_encoder_tokens,input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers,num_epochs,output_size,input_size_decoder,batch_size,beam_width):\n    \n    criterion = nn.CrossEntropyLoss()\n    data_train = torch.split(input_data,batch_size,dim=1)\n    target_train = torch.split(target_data,batch_size,dim=1)\n    learning_rate=0.001\n    if(cell_type==\"LSTM\"):\n        encoder_net = Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp,output_size).to(device)\n        decoder_net = Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n\n        model = Seq2Seq(encoder_net, decoder_net).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif(cell_type==\"RNN\" or cell_type ==\"GRU\"):\n        if(cell_type==\"RNN\"):\n            encoder_net = EncoderRNN(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers, dp,output_size).to(device)\n            decoder_net = DecoderRNN(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n        else:\n            encoder_net = EncoderGRU(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers, dp,output_size).to(device)\n            decoder_net = DecoderGRU(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n            \n        model = Seq2SeqGR(encoder_net, decoder_net).to(device)    \n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        #print(train_ds_x)\n    total_words_val = len(X_val)\n    total_words_test = len(X_test)\n        \n    for epoch in range(num_epochs):\n        correct_prediction = 0\n        print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n        model.eval()\n        model.train()\n        dataset_Size = len(data_train)\n        for i in range(dataset_Size):\n            neg = -1\n            pos = 1\n            x = data_train[i]\n            y = target_train[i]\n            # Get input and targets and get to cuda\n            inp_data = x.to(device)\n            target = y.to(device)\n\n            # Forward prop\n            output = model(inp_data, target)\n            size = output.shape[2]\n            temp = output[pos:]\n            temp1 =temp.reshape(neg, size)\n            optimizer.zero_grad()\n            output = temp1\n            tar = target[pos:]\n            target = tar.reshape(neg)\n\n            \n            loss = criterion(output, target)\n\n                # Back prop\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n            # Gradient descent step\n            optimizer.step()\n\n        model.eval()\n        if(cell_type==\"LSTM\"):\n            for i in range(total_words_val):\n       \n                decoded_sentence,_ = beam_find(model,X_val[i], 10,input_char_index, output_char_index, reverse_input_char_index, \n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n                num_encoder_tokens, num_decoder_tokens,beam_width,device)\n                true_output = Y_val[i][1:-1]\n                if true_output== decoded_sentence:\n                        correct_prediction = correct_prediction + 1\n       \n            \n        elif(cell_type==\"GRU\" or cell_type==\"RNN\"):\n            for i in range(total_words_val):\n                 decoded_sentence,_ = beam_findGR(model,X_val[i], 10,input_char_index, output_char_index, reverse_input_char_index, \n                 reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n                 num_encoder_tokens, num_decoder_tokens,beam_width,device)\n                 true_output = Y_val[i][1:-1]\n                 if true_output== decoded_sentence:\n                        correct_prediction = correct_prediction + 1\n        test_accuracy = correct_prediction / total_words_val\n        print(test_accuracy)\n        wandb.log({'val_accuracy_with_attention' : test_accuracy*100})\n\n   ","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.330963Z","iopub.execute_input":"2023-05-21T09:12:02.331379Z","iopub.status.idle":"2023-05-21T09:12:02.353796Z","shell.execute_reply.started":"2023-05-21T09:12:02.331343Z","shell.execute_reply":"2023-05-21T09:12:02.352969Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key='17d991db26320e751b170877037d1067a164fe6d')\nwandb.init(project=\"Assignment_3_Fina\",name=\"Question_5\")","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:02.355018Z","iopub.execute_input":"2023-05-21T09:12:02.355439Z","iopub.status.idle":"2023-05-21T09:12:35.771259Z","shell.execute_reply.started":"2023-05-21T09:12:02.355400Z","shell.execute_reply":"2023-05-21T09:12:35.770414Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m046\u001b[0m (\u001b[33mharshvrma\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_091204-qkaml33m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_3_Fina/runs/qkaml33m' target=\"_blank\">Question_5</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_3_Fina' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshvrma/Assignment_3_Fina' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_Fina</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshvrma/Assignment_3_Fina/runs/qkaml33m' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_Fina/runs/qkaml33m</a>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/harshvrma/Assignment_3_Fina/runs/qkaml33m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f5c665c76a0>"},"metadata":{}}]},{"cell_type":"code","source":"#training(input_size_encoder,256,0.4,\"LSTM\",256,1,1,5,num_decoder_tokens,num_decoder_tokens,32,1)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:35.775365Z","iopub.execute_input":"2023-05-21T09:12:35.777874Z","iopub.status.idle":"2023-05-21T09:12:35.784829Z","shell.execute_reply.started":"2023-05-21T09:12:35.777838Z","shell.execute_reply":"2023-05-21T09:12:35.784013Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sweep_config2 = {\n    'method': 'bayes',\n    'metric': {'goal': 'maximize', 'name': 'val_accuracy_with_attention'},\n    'parameters': {'embedding_size_input': {'values': [128, 256, 512]},\n                   'hidden_size_EncDec': {'values': [128, 256, 512]},\n                   'cell_type_value': {'values': ['LSTM','GRU','RNN']},\n                   'batch_size_value': {'values': [128,256,512]},\n                   'dropout_EncDec': {'values': [0.1, 0.2, 0.3, 0.4]},\n                   'Number_Epochs' :{'values':[10,20,30,40]},\n                   'beam_width':{'values':[1,2,3,4,5]}\n                }}\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:35.788977Z","iopub.execute_input":"2023-05-21T09:12:35.791536Z","iopub.status.idle":"2023-05-21T09:12:35.801262Z","shell.execute_reply.started":"2023-05-21T09:12:35.791500Z","shell.execute_reply":"2023-05-21T09:12:35.800455Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def train():\n    var1 = wandb.init()\n    var2 = var1.config\n    embedding_size_input = var2.embedding_size_input\n    dropout_enc_dec = var2.dropout_EncDec\n    cell_type_vl = var2.cell_type_value\n    hidden_size_val = var2.hidden_size_EncDec\n    number_of_layers = 1\n    number_of_epochs = var2.Number_Epochs\n    batch_size = var2.batch_size_value\n    beam_width = var2.beam_width\n    \n    training(input_size_encoder , embedding_size_input,  dropout_enc_dec, cell_type_vl, hidden_size_val,  number_of_layers,   number_of_layers,number_of_epochs,num_decoder_tokens,num_decoder_tokens, batch_size,beam_width)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:35.806365Z","iopub.execute_input":"2023-05-21T09:12:35.808916Z","iopub.status.idle":"2023-05-21T09:12:35.817641Z","shell.execute_reply.started":"2023-05-21T09:12:35.808884Z","shell.execute_reply":"2023-05-21T09:12:35.816831Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config2, project=\"Assignment_3_\")\nwandb.agent(sweep_id, train, count=5)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:12:35.822489Z","iopub.execute_input":"2023-05-21T09:12:35.825330Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: tl7nc3se\nSweep URL: https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se\n","output_type":"stream"},{"name":"stderr","text":"wandb: Waiting for W&B process to finish... (success).\nwandb: üöÄ View run Question_5 at: https://wandb.ai/harshvrma/Assignment_3_Fina/runs/qkaml33m\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20230521_091204-qkaml33m/logs\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ut5utksv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tNumber_Epochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size_value: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type_value: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_EncDec: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size_input: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size_EncDec: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_091309-ut5utksv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_3_/runs/ut5utksv' target=\"_blank\">fearless-sweep-1</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_3_' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshvrma/Assignment_3_' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshvrma/Assignment_3_/runs/ut5utksv' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/runs/ut5utksv</a>"},"metadata":{}},{"name":"stdout","text":"[Epoch 0 / 30]\n0.008544921875\n[Epoch 1 / 30]\n0.0380859375\n[Epoch 2 / 30]\n0.087158203125\n[Epoch 3 / 30]\n0.123291015625\n[Epoch 4 / 30]\n0.155517578125\n[Epoch 5 / 30]\n0.179443359375\n[Epoch 6 / 30]\n0.2158203125\n[Epoch 7 / 30]\n0.244140625\n[Epoch 8 / 30]\n0.2607421875\n[Epoch 9 / 30]\n0.2587890625\n[Epoch 10 / 30]\n0.31298828125\n[Epoch 11 / 30]\n0.31201171875\n[Epoch 12 / 30]\n0.332275390625\n[Epoch 13 / 30]\n0.35302734375\n[Epoch 14 / 30]\n0.3662109375\n[Epoch 15 / 30]\n0.380126953125\n[Epoch 16 / 30]\n0.393310546875\n[Epoch 17 / 30]\n0.3837890625\n[Epoch 18 / 30]\n0.39599609375\n[Epoch 19 / 30]\n0.41552734375\n[Epoch 20 / 30]\n0.406005859375\n[Epoch 21 / 30]\n0.407470703125\n[Epoch 22 / 30]\n0.415771484375\n[Epoch 23 / 30]\n0.41650390625\n[Epoch 24 / 30]\n0.4248046875\n[Epoch 25 / 30]\n0.433349609375\n[Epoch 26 / 30]\n0.4365234375\n[Epoch 27 / 30]\n0.4365234375\n[Epoch 28 / 30]\n0.427978515625\n[Epoch 29 / 30]\n0.432861328125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>val_accuracy_with_attention</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>val_accuracy_with_attention</td><td>43.28613</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fearless-sweep-1</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_3_/runs/ut5utksv' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/runs/ut5utksv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230521_091309-ut5utksv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jb49d6kz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tNumber_Epochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size_value: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type_value: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_EncDec: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size_input: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size_EncDec: 128\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_110205-jb49d6kz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_3_/runs/jb49d6kz' target=\"_blank\">azure-sweep-2</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_3_' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshvrma/Assignment_3_' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshvrma/Assignment_3_/runs/jb49d6kz' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/runs/jb49d6kz</a>"},"metadata":{}},{"name":"stdout","text":"[Epoch 0 / 30]\n0.000244140625\n[Epoch 1 / 30]\n0.005126953125\n[Epoch 2 / 30]\n0.01025390625\n[Epoch 3 / 30]\n0.017333984375\n[Epoch 4 / 30]\n0.026123046875\n[Epoch 5 / 30]\n0.0400390625\n[Epoch 6 / 30]\n0.04296875\n[Epoch 7 / 30]\n0.0615234375\n[Epoch 8 / 30]\n0.0673828125\n[Epoch 9 / 30]\n0.083251953125\n[Epoch 10 / 30]\n0.09228515625\n[Epoch 11 / 30]\n0.093994140625\n[Epoch 12 / 30]\n0.111572265625\n[Epoch 13 / 30]\n0.11474609375\n[Epoch 14 / 30]\n0.12744140625\n[Epoch 15 / 30]\n0.136962890625\n[Epoch 16 / 30]\n0.136962890625\n[Epoch 17 / 30]\n0.154052734375\n[Epoch 18 / 30]\n0.126708984375\n[Epoch 19 / 30]\n0.169189453125\n[Epoch 20 / 30]\n0.1279296875\n[Epoch 21 / 30]\n0.181396484375\n[Epoch 22 / 30]\n0.1728515625\n[Epoch 23 / 30]\n0.1884765625\n[Epoch 24 / 30]\n0.2119140625\n[Epoch 25 / 30]\n0.192626953125\n[Epoch 26 / 30]\n0.215087890625\n[Epoch 27 / 30]\n0.236328125\n[Epoch 28 / 30]\n0.21142578125\n[Epoch 29 / 30]\n0.2392578125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d691bbdd5f3e4358943cad64b538ec35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>val_accuracy_with_attention</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>val_accuracy_with_attention</td><td>23.92578</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">azure-sweep-2</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_3_/runs/jb49d6kz' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/runs/jb49d6kz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230521_110205-jb49d6kz/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6tnk8tgg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tNumber_Epochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size_value: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type_value: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_EncDec: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size_input: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size_EncDec: 128\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_112707-6tnk8tgg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_3_/runs/6tnk8tgg' target=\"_blank\">rosy-sweep-3</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_3_' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshvrma/Assignment_3_' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/sweeps/tl7nc3se</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshvrma/Assignment_3_/runs/6tnk8tgg' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_/runs/6tnk8tgg</a>"},"metadata":{}},{"name":"stdout","text":"[Epoch 0 / 20]\n0.0\n[Epoch 1 / 20]\n0.001220703125\n[Epoch 2 / 20]\n0.008544921875\n[Epoch 3 / 20]\n0.0205078125\n[Epoch 4 / 20]\n0.029296875\n[Epoch 5 / 20]\n0.043212890625\n[Epoch 6 / 20]\n0.061279296875\n[Epoch 7 / 20]\n0.072998046875\n[Epoch 8 / 20]\n0.083984375\n[Epoch 9 / 20]\n0.109375\n[Epoch 10 / 20]\n0.11865234375\n[Epoch 11 / 20]\n0.123291015625\n[Epoch 12 / 20]\n0.140625\n[Epoch 13 / 20]\n0.148193359375\n[Epoch 14 / 20]\n0.162841796875\n[Epoch 15 / 20]\n0.172119140625\n[Epoch 16 / 20]\n0.181396484375\n[Epoch 17 / 20]\n0.18701171875\n[Epoch 18 / 20]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}