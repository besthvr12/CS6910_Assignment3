{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport csv\nimport matplotlib as mlp\nimport torch.nn as nn\nimport numpy as np\nimport torch.optim as optim\nimport os\nimport random\nimport torch\nimport PIL\nimport pandas as pd\nimport cv2\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\nimport pathlib\nimport torch.nn.functional as F\nimport glob\nimport matplotlib.pyplot as plt\nimport shutil\nimport matplotlib.ticker as ticker\nimport heapq\nfrom collections import defaultdict\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-21T09:22:54.125132Z","iopub.execute_input":"2023-05-21T09:22:54.125482Z","iopub.status.idle":"2023-05-21T09:22:58.522243Z","shell.execute_reply.started":"2023-05-21T09:22:54.125454Z","shell.execute_reply":"2023-05-21T09:22:58.521367Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = []\nY_train = []\nX_val = []\nY_val = []\nX_test = []\nY_test = []","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:58.528503Z","iopub.execute_input":"2023-05-21T09:22:58.530413Z","iopub.status.idle":"2023-05-21T09:22:58.536273Z","shell.execute_reply.started":"2023-05-21T09:22:58.530372Z","shell.execute_reply":"2023-05-21T09:22:58.535135Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_train.csv\")\nread_tsv = csv.reader(tsv_file)\n\npad1 = \"\\t\"\nspace1 = \"\\n\"\nfor i in read_tsv:   \n    Y_train.append(i[1])\n    X_train.append(i[0])\n\nX_train = np.array(X_train)\n\nval_tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_valid.csv\")\nval_read_tsv = csv.reader(val_tsv_file)\npad2 = \"\\t\"\nspace2 = \"\\n\"\nfor i in val_read_tsv:\n    Y_val.append(i[1])\n    X_val.append(i[0])\nX_val= np.array(X_val)\n\ntest_tsv_file = open(\"/kaggle/input/assignment3/aksharantar_sampled/tel/tel_test.csv\")\ntest_read_tsv = csv.reader(test_tsv_file)\npad3 = \"\\t\"\nspace3 = \"\\n\"\nfor i in test_read_tsv:\n    Y_test.append(i[1])\n    X_test.append(i[0])\n\nX_test = np.array(X_test)\ntrainsize=len(X_train)\ntestsize = len(X_test)\nvalidationsize = len(X_val)\n\nprint(\"Number of Training samples:\", trainsize)\nprint(\"Number of Test Samples:\" , testsize)\nprint(\"Number of Validation Samples:\", validationsize)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:58.538283Z","iopub.execute_input":"2023-05-21T09:22:58.538751Z","iopub.status.idle":"2023-05-21T09:22:58.658435Z","shell.execute_reply.started":"2023-05-21T09:22:58.538623Z","shell.execute_reply":"2023-05-21T09:22:58.657218Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of Training samples: 51200\nNumber of Test Samples: 4096\nNumber of Validation Samples: 4096\n","output_type":"stream"}]},{"cell_type":"code","source":"Y_train = np.array(Y_train)\nfor i in range(Y_train.shape[0]):\n    Y_train[i] = pad1 + Y_train[i] + space1\n\nY_val = np.array(Y_val)\nfor i in range(Y_val.shape[0]):\n    Y_val[i] = pad2 + Y_val[i] + space2\n    \nY_test = np.array(Y_test)\nfor i in range(Y_test.shape[0]):\n    Y_test[i] = pad3+ Y_test[i] + space3\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:58.662179Z","iopub.execute_input":"2023-05-21T09:22:58.662536Z","iopub.status.idle":"2023-05-21T09:22:58.765231Z","shell.execute_reply.started":"2023-05-21T09:22:58.662508Z","shell.execute_reply":"2023-05-21T09:22:58.764386Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"traintargetsize=len(Y_train)\ntesttargetsize = len(Y_test)\ntesttargetsize = len(Y_val)\nprint(\"TrainSize of Target \",traintargetsize)\nprint(\"TrainSize of Target \",testtargetsize)\nprint(\"TrainSize of Target \",testtargetsize)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:58.766492Z","iopub.execute_input":"2023-05-21T09:22:58.766823Z","iopub.status.idle":"2023-05-21T09:22:58.776220Z","shell.execute_reply.started":"2023-05-21T09:22:58.766798Z","shell.execute_reply":"2023-05-21T09:22:58.773976Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"TrainSize of Target  51200\nTrainSize of Target  4096\nTrainSize of Target  4096\n","output_type":"stream"}]},{"cell_type":"code","source":"input_corpus = set()\noutput_corpus = set()\ninput_corpus = set(char for word in X_train for char in word if char not in input_corpus)\noutput_corpus = set(char for word in Y_train for char in word if char not in output_corpus)\ninput_corpus.add(\" \")\noutput_corpus.add(\" \")\ninput_corpus = sorted(list(input_corpus))\noutput_corpus = sorted(list(output_corpus))\nval_input_corpus = set()\nval_output_corpus = set()\nmax_encoder_seq_length = max([len(txt) for txt in X_train]) + 2\nmax_decoder_seq_length = max([len(txt) for txt in Y_train])\n\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)\nval_input_corpus = set(char for word in X_val for char in word if char not in val_input_corpus)\nval_output_corpus = set(char for word in Y_val for char in word if char not in val_output_corpus)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:58.778090Z","iopub.execute_input":"2023-05-21T09:22:58.778447Z","iopub.status.idle":"2023-05-21T09:22:59.075542Z","shell.execute_reply.started":"2023-05-21T09:22:58.778412Z","shell.execute_reply":"2023-05-21T09:22:59.074592Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Max sequence length for inputs: 30\nMax sequence length for outputs: 21\n","output_type":"stream"}]},{"cell_type":"code","source":"num_encoder_tokens = len(input_corpus)\nnum_decoder_tokens = len(output_corpus)\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:59.078620Z","iopub.execute_input":"2023-05-21T09:22:59.078927Z","iopub.status.idle":"2023-05-21T09:22:59.085831Z","shell.execute_reply.started":"2023-05-21T09:22:59.078902Z","shell.execute_reply":"2023-05-21T09:22:59.084733Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of unique input tokens: 27\nNumber of unique output tokens: 65\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\ninput_data = np.zeros((max_encoder_seq_length,len(X_train)), dtype=\"int64\")\ntarget_data = np.zeros((max_decoder_seq_length,len(X_train)), dtype=\"int64\")\ninput_data_val = np.zeros((max_encoder_seq_length,len(X_val)), dtype=\"int64\")\ntarget_data_val = np.zeros((max_decoder_seq_length,len(X_val)), dtype=\"int64\")","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:59.087503Z","iopub.execute_input":"2023-05-21T09:22:59.088281Z","iopub.status.idle":"2023-05-21T09:22:59.095855Z","shell.execute_reply.started":"2023-05-21T09:22:59.088229Z","shell.execute_reply":"2023-05-21T09:22:59.094926Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"input_size_encoder = num_encoder_tokens\ninput_size_decoder = num_decoder_tokens\noutput_size = num_decoder_tokens\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:59.097507Z","iopub.execute_input":"2023-05-21T09:22:59.097921Z","iopub.status.idle":"2023-05-21T09:22:59.105885Z","shell.execute_reply.started":"2023-05-21T09:22:59.097823Z","shell.execute_reply":"2023-05-21T09:22:59.105067Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def create_char_index(corpus):\n    char_index = defaultdict(int)\n    for i, char in enumerate(corpus):\n        if char not in char_index:\n            char_index[char] = i\n    return dict(char_index)\n\ninput_char_index = create_char_index(input_corpus)\noutput_char_index = create_char_index(output_corpus)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:59.109058Z","iopub.execute_input":"2023-05-21T09:22:59.109347Z","iopub.status.idle":"2023-05-21T09:22:59.117199Z","shell.execute_reply.started":"2023-05-21T09:22:59.109323Z","shell.execute_reply":"2023-05-21T09:22:59.116228Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(X_train)):\n    x = X_train[i]\n    y = Y_train[i]\n    count=0\n    size = x.size\n    for t, char in enumerate(x):\n        input_data[t, i] = input_char_index[char]\n    count=count+1 \n    input_data[t + 1 :,i] = input_char_index[\" \"]\n    count1=1\n    for t, char in enumerate(y):\n        target_data[t, i] = output_char_index[char]\n        count1=count+1     \n    target_data[t + 1 :,i] = output_char_index[\" \"]\n    \n\ndata_type = torch.int64\ndata_type1=torch.float64\nfor i in range(len(X_val)):\n    x=X_val[i]\n    y=Y_val[i]\n    count = 0\n    size - x.size\n    for t, char in enumerate(x):\n        input_data_val[t, i] = input_char_index[char]\n        count = count +1\n    input_data_val[t + 1 :,i] = input_char_index[\" \"]\n    \n    count = 0\n    for t, char in enumerate(y):\n        target_data_val[t, i] = output_char_index[char]\n        count = count +1\n            \n    target_data_val[t + 1 :,i] = output_char_index[\" \"]\n\ninput_data_val = torch.tensor(input_data_val,dtype=data_type)\ntarget_data_val = torch.tensor(target_data_val,dtype=data_type)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:22:59.119666Z","iopub.execute_input":"2023-05-21T09:22:59.120658Z","iopub.status.idle":"2023-05-21T09:23:00.252865Z","shell.execute_reply.started":"2023-05-21T09:22:59.120620Z","shell.execute_reply":"2023-05-21T09:23:00.251836Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninput_data = torch.tensor(input_data,dtype=data_type)\nreverse_input_char_index = {i: char for char, i in input_char_index.items()}\nreverse_target_char_index = {i: char for char, i in output_char_index.items()}\ntarget_data = torch.tensor(target_data,dtype=data_type)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:23:00.255624Z","iopub.execute_input":"2023-05-21T09:23:00.256378Z","iopub.status.idle":"2023-05-21T09:23:00.278535Z","shell.execute_reply.started":"2023-05-21T09:23:00.256341Z","shell.execute_reply":"2023-05-21T09:23:00.277465Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#LSTM RUN Only\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout,opsize):\n        super(Encoder, self).__init__()\n        self.dropout,self.num_layers,self.bidirectional,self.embedding = nn.Dropout(dropout),num_layers,True,nn.Embedding(input_size, embedding_size)\n        self.hidden_size,self.identity_init,self.init_h0 = hidden_size,torch.eye(hidden_size),nn.Linear(num_layers * 2, num_layers)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n        self.fc_hidden,self.fc_cell = nn.Linear(hidden_size * 2, hidden_size), nn.Linear(hidden_size * 2, hidden_size)     \n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n        a = self.embedding(x)\n        embedding = self.dropout(a)\n        identity_init = self.identity_init\n        embedded = self.embedding(x).view(1, 1, -1)\n        # embedding shape: (seq_length, N, embedding_size)\n\n        outputs, (hidden, cell) = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n        init_h0 = self.init_h0\n\n        return hidden, cell\n    def initHidden(self):\n        value = 1\n        return torch.zeros(value, value, self.hidden_size, device=device)\nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(Decoder, self).__init__()\n        self.dropout,self.num_layers,self.bidirectional,self.embedding = nn.Dropout(dropout), num_layers,True,nn.Embedding(input_size, embedding_size)\n        self.hidden_size,self.identity_init,self.init_h0 = hidden_size,torch.eye(hidden_size), nn.Linear(num_layers * 2, num_layers)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=0)\n        self.relu = nn.ReLU()\n\n    def forward(self, x, hidden, cell):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n        hid=hidden.squeeze(0)\n        embedding = self.dropout(self.embedding(x))\n        embed=embedding[0:]\n        # embedding shape: (1, N, embedding_size)\n\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        # outputs shape: (1, N, hidden_size)\n\n        predictions = self.fc(outputs).squeeze(0)\n        results=predictions[0:]\n        results = self.softmax(predictions)\n        # predictions shape: (1, N, length_target_vocabulary) to send it to\n        # loss function we want it to be (N, length_target_vocabulary) so we're\n        # just gonna remove the first dim\n       # predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.2):\n        x = target[0]\n        #batch_size = source.shape[1]\n        #target_len = target.shape[0]\n        variables = 0\n        target_vocab_size = num_decoder_tokens\n\n        outputs = torch.zeros(target.shape[0], source.shape[1], target_vocab_size).to(device)\n        \n        hidden, cell=self.encoder(source)\n        ht = hidden[-1, :, :]\n       # hidden, cell = self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n        res = cell[-1,:,:]\n\n        for t in range(1, target.shape[0]):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden, cell = self.decoder(x, hidden, cell)\n\n            # Store next output prediction\n            outputs[t] = output\n            res = output[0:]\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n            #x = target[t] if random.random() < teacher_force_ratio else best_guess\n            if random.random() < teacher_force_ratio:\n                x = target[t]\n            else:\n                x = best_guess\n\n        return outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:23:00.283158Z","iopub.execute_input":"2023-05-21T09:23:00.283453Z","iopub.status.idle":"2023-05-21T09:23:00.305589Z","shell.execute_reply.started":"2023-05-21T09:23:00.283427Z","shell.execute_reply":"2023-05-21T09:23:00.304497Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#For GRU\nclass GRU_Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout,size):\n        super(GRU_Encoder, self).__init__()\n        self.dropout,self.num_layers,self.bidirectional,self.embedding = nn.Dropout(dropout),num_layers,True,nn.Embedding(input_size, embedding_size)\n        self.hidden_size,self.identity_init,self.init_h0 = hidden_size,torch.eye(hidden_size),nn.Linear(num_layers * 2, num_layers)\n        self.tanh = nn.LeakyReLU()\n        self.fc_hidden,self.fc_cell = nn.Linear(hidden_size * 2, hidden_size), nn.Linear(hidden_size * 2, hidden_size) \n        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n\n        a = self.embedding(x)\n        embedding = self.dropout(a)\n        identity_init = self.identity_init\n        embedded = self.embedding(x)\n        #embedding = self.dropout(self.embedding(x))\n        # embedding shape: (seq_length, N, embedding_size)\n        outputs, hidden = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n        # hidden shape: (num_layers, N, hidden_size)\n        ht=hidden[0: :]\n        return hidden\n\nclass GRU_Decoder(nn.Module):\n    def __init__(\n        self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(GRU_Decoder, self).__init__()\n        self.dropout,self.num_layers,self.bidirectional,self.embedding = nn.Dropout(dropout), num_layers,True,nn.Embedding(input_size, embedding_size)\n        self.hidden_size,self.identity_init,self.init_h0 = hidden_size,torch.eye(hidden_size), nn.Linear(num_layers * 2, num_layers)\n        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=0)\n        self.relu = nn.ReLU()\n\n       # self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x, hidden):\n\n        x = x.unsqueeze(0)\n        a = self.embedding(x)\n        embedding = self.dropout(a)\n        # embedding shape: (1, N, embedding_size)\n\n        outputs, hidden = self.rnn(embedding, hidden)\n        \n        predictions = self.fc(outputs)\n        results = predictions\n        results= self.softmax(results)\n        predictions = predictions.squeeze(0)\n        return predictions, hidden\n\n\nclass Seq2SeqGR(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2SeqGR, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.2):\n        target_vocab_size = num_decoder_tokens\n        count= 0 \n        outputs = torch.zeros(target.shape[0], source.shape[1], target_vocab_size).to(device)\n        source_vocab_size = num_encoder_tokens\n\n        hidden= self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n        x = target[0]\n\n        for t in range(1, target.shape[0]):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden = self.decoder(x, hidden)\n\n            # Store next output prediction\n            outputs[t] = output\n\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            res = output[0:]\n            best_guess = output.argmax(1)\n            #x = target[t] if random.random() < teacher_force_ratio else best_guess\n            if random.random() < teacher_force_ratio:\n                x = target[t]\n            else:\n                x = best_guess\n\n        return outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:23:00.307986Z","iopub.execute_input":"2023-05-21T09:23:00.308791Z","iopub.status.idle":"2023-05-21T09:23:00.329723Z","shell.execute_reply.started":"2023-05-21T09:23:00.308755Z","shell.execute_reply":"2023-05-21T09:23:00.328534Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"## This cell is to Run RNN\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n        super(EncoderRNN, self).__init__()\n        self.dropout,self.num_layers,self.bidirectional,self.embedding = nn.Dropout(dropout),num_layers,True,nn.Embedding(input_size, embedding_size)\n        self.hidden_size,self.identity_init,self.init_h0 = hidden_size,torch.eye(hidden_size),nn.Linear(num_layers * 2, num_layers)\n        self.tanh = nn.LeakyReLU()\n        self.fc_hidden,self.fc_cell = nn.Linear(hidden_size * 2, hidden_size), nn.Linear(hidden_size * 2, hidden_size) \n        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n        a = self.embedding(x)\n        embedding = self.dropout(a)\n        identity_init = self.identity_init\n        embedded = self.embedding(x)\n        #embedding = self.dropout(self.embedding(x))\n        # embedding shape: (seq_length, N, embedding_size)\n        outputs, hidden = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n        # hidden shape: (num_layers, N, hidden_size)\n        return hidden\n    \n\n\n\n\n##For RNN\nclass DecoderRNN(nn.Module):\n    def __init__(\n        self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n        super(DecoderRNN, self).__init__()\n      #  self.softmax = nn.LogSoftmax(dim=1)\n        self.dropout,self.num_layers,self.bidirectional,self.embedding = nn.Dropout(dropout), num_layers,True,nn.Embedding(input_size, embedding_size)\n        self.hidden_size,self.identity_init,self.init_h0 = hidden_size,torch.eye(hidden_size), nn.Linear(num_layers * 2, num_layers)\n        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=0)\n        self.relu = nn.ReLU()\n\n\n    def forward(self, x, hidden):\n\n        x = x.unsqueeze(0)\n        a= self.embedding(x)\n\n        embedding = self.dropout(a)\n\n        outputs, hidden = self.rnn(embedding, hidden)\n        \n        predictions = self.fc(outputs)\n        results = predictions\n        results= self.softmax(results)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:23:00.331729Z","iopub.execute_input":"2023-05-21T09:23:00.332474Z","iopub.status.idle":"2023-05-21T09:23:00.347054Z","shell.execute_reply.started":"2023-05-21T09:23:00.332389Z","shell.execute_reply":"2023-05-21T09:23:00.346120Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import heapq\n#     return word_t\ndef beam_findGR(model, word, norm,input_char_index, output_char_index, reverse_input_char_index,\n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n\n    word_t = ''\n\n    word_val = 0\n    data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n    datatype = torch.int64\n    for t in range(len(word)):\n        char = word[t]\n        data[t, word_val] = input_char_index[char]\n    word_size =len(word)\n    data[ word_size:, word_val] = input_char_index[\" \"]\n    data = torch.tensor(data, dtype=datatype).to(device)\n\n    with torch.no_grad():\n        hidden = model.encoder(data)\n\n    # Initialize beam\n    \n    value = 1\n    ovalue=-1\n    padding = '\\t'\n    initial_sequence = torch.tensor(np.array(output_char_index[padding]).reshape(value,)).to(device)\n    zero = 0\n    res =hidden.unsqueeze(0)\n    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n    start ='\\n'\n    for _ in range(max_decoder_seq_length):\n        candidates = []\n        options=[]\n        for score, seq, hidden in beam:\n            count = 0\n            last_token = seq[ovalue].item()\n            if last_token == output_char_index[start]:\n                # If the sequence ends with the end token, add it to the candidates\n                candidates.append((score, seq, hidden))\n                count =count+1\n                continue\n\n            x = torch.tensor(np.array(last_token).reshape(value,)).to(device)\n            output, hidden = model.decoder(x, hidden.squeeze(0))\n            res = output[0:]\n            probabilities = F.softmax(output, dim=1)\n            result =probabilities\n            # Get the top-k probabilities and tokens\n            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n            tops = topk_probs[0]\n            topk = topk_tokens[0]\n            for prob, token in zip(tops, topk):\n                new_s= token.unsqueeze(0)\n                new_seq = torch.cat((seq, new_s), dim=0)\n                new_hidden = hidden.clone().unsqueeze(0)\n                penalty = len(new_seq) - 1\n                len_pen = length_penalty\n                length_penalty_factor = ((penalty) / 5) **  len_pen   # Adjust penalty factor as needed\n                candidates.append((score + torch.log(prob).item() / length_penalty_factor, new_seq, new_hidden))\n                options.append((score + torch.log(prob).item() / length_penalty_factor))\n        # Select top-k candidates based on the accumulated scores\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n    # Select the best sequence from the beam as the output\n    best_res = \"\"\n    best_score, best_sequence, _ = max(beam, key=lambda x: x[zero])\n    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[value:ovalue]])\n    best_res = word_t\n\n    return word_t","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:23:00.350163Z","iopub.execute_input":"2023-05-21T09:23:00.350534Z","iopub.status.idle":"2023-05-21T09:23:00.368018Z","shell.execute_reply.started":"2023-05-21T09:23:00.350509Z","shell.execute_reply":"2023-05-21T09:23:00.367096Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def beam_find(model, word,norm,input_char_index, output_char_index, reverse_input_char_index,\n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n\n    word_t = ''\n\n    # Encode the input word\n    word_val = 0\n    data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n    datatype = torch.int64\n    for t in range(len(word)):\n        char = word[t]\n        data[t,  word_val] = input_char_index[char]\n    word_size = len(word)\n    data[word_size:, word_val] = input_char_index[\" \"]\n    data = torch.tensor(data, dtype=torch.int64).to(device)\n\n    with torch.no_grad():\n        hidden,cell = model.encoder(data)\n\n    # Initialize beam\n   \n    value = 1\n    ovalue=-1\n    padding = '\\t'\n    initial_sequence = torch.tensor(np.array(output_char_index[padding]).reshape(value,)).to(device)\n    zero = 0\n    res =hidden.unsqueeze(0)\n    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n    start ='\\n'\n    for _ in range(max_decoder_seq_length):\n        candidates = []\n        for score, seq, hidden in beam:\n            count = 0\n            last_token = seq[ovalue].item()\n            if last_token == output_char_index[start]:\n                # If the sequence ends with the end token, add it to the candidates\n                candidates.append((score, seq, hidden))\n                count =count+1\n                continue\n\n            x = torch.tensor(np.array(last_token).reshape(value,)).to(device)\n            output, hidden,cell = model.decoder(x, hidden.squeeze(0),cell)\n            res = output[0:]\n            probabilities = F.softmax(output, dim=1)\n            result =probabilities\n            # Get the top-k probabilities and tokens\n            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n            tops = topk_probs[0]\n            topk = topk_tokens[0]\n            options=[]\n            for prob, token in zip(tops, topk):\n                new_s= token.unsqueeze(0)\n                new_seq = torch.cat((seq, new_s), dim=0)\n                new_hidden = hidden.clone().unsqueeze(0)\n                penalty = len(new_seq) - 1\n                len_pen = length_penalty\n                length_penalty_factor = ((penalty) / 5) **  len_pen   # Adjust penalty factor as needed\n                candidates.append((score + torch.log(prob).item() / length_penalty_factor, new_seq, new_hidden))\n                options.append((score + torch.log(prob).item() / length_penalty_factor))\n        # Select top-k candidates based on the accumulated scores\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n    # Select the best sequence from the beam as the output\n    best_res = \"\"\n    best_score, best_sequence, _ = max(beam, key=lambda x: x[zero])\n    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[value:ovalue]])\n    best_res =word_t\n\n    return word_t","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:42:52.576337Z","iopub.execute_input":"2023-05-21T09:42:52.576768Z","iopub.status.idle":"2023-05-21T09:42:52.597251Z","shell.execute_reply.started":"2023-05-21T09:42:52.576729Z","shell.execute_reply":"2023-05-21T09:42:52.596266Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training(num_encoder_tokens,input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers,num_epochs,output_size,input_size_decoder,batch_size,beam_width):\n    \n    criterion = nn.CrossEntropyLoss()\n    data_train = torch.split(input_data,batch_size,dim=1)\n    target_train = torch.split(target_data,batch_size,dim=1)\n    learning_rate=0.001\n    if(cell_type==\"LSTM\"):\n        encoder_net = Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp,output_size).to(device)\n        decoder_net = Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n\n        model = Seq2Seq(encoder_net, decoder_net).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif(cell_type==\"RNN\" or cell_type ==\"GRU\"):\n        if(cell_type==\"RNN\"):\n            encoder_net = EncoderRNN(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers, dp).to(device)\n            decoder_net = DecoderRNN(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n        else:\n            encoder_net = GRU_Encoder(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers, dp,output_size).to(device)\n            decoder_net = GRU_Decoder(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n        total_words = len(X_val)\n       \n        model = Seq2SeqGR(encoder_net, decoder_net).to(device)    \n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        \n        \n    for epoch in range(num_epochs):\n        correct_output = 0\n        print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n        model.eval()\n        model.train()\n\n        train_size = len(data_train)\n        for i in range(train_size):\n            neg = -1\n            pos = 1\n            a = data_train[i]\n            b = target_train[i]\n            # Get input and targets and get to cuda\n            inp_data = a.to(device)\n            target = b.to(device)\n\n            # Forward prop\n            output = model(inp_data, target)\n            size = output.shape[2]\n            temp = output[pos:]\n            temp1 =temp.reshape(neg, size)\n\n            output = temp1\n            tar = target[pos:]\n            target = tar.reshape(neg)\n\n            optimizer.zero_grad()\n            loss = criterion(output, target)\n\n            # Back prop\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n                # Gradient descent step\n            optimizer.step()\n\n        model.eval()\n        total_words = len(X_val)\n        total_words_test = len(X_test)\n        if(cell_type==\"LSTM\"):\n            for i in range(total_words):\n       \n                decoded_sentence = beam_find(model,X_val[i], 10,input_char_index, output_char_index, reverse_input_char_index, \n                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n                num_encoder_tokens, num_decoder_tokens,beam_width,device)\n                true_output = Y_val[i][1:-1]\n                if true_output== decoded_sentence:\n                     correct_output = correct_output + 1\n       \n            \n        elif(cell_type==\"GRU\" or cell_type==\"RNN\"):\n            for i in range(total_words):\n                    decoded_sentence = beam_findGR(model,X_val[i], 10,input_char_index, output_char_index, reverse_input_char_index, \n                    reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n                    num_encoder_tokens, num_decoder_tokens,beam_width,device)\n                    true_output = Y_val[i][1:-1]\n                    if true_output== decoded_sentence:\n                        correct_output = correct_output + 1\n        Calculated_test_accuracy = correct_output / total_words\n        print(Calculated_test_accuracy)\n        wandb.log({'val_accuracy' : Calculated_test_accuracy*100})\n\n   ","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:42:54.382310Z","iopub.execute_input":"2023-05-21T09:42:54.382750Z","iopub.status.idle":"2023-05-21T09:42:54.410591Z","shell.execute_reply.started":"2023-05-21T09:42:54.382693Z","shell.execute_reply":"2023-05-21T09:42:54.408845Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key='17d991db26320e751b170877037d1067a164fe6d')\nwandb.init(project=\"Assignment_3_F\")","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:42:55.221319Z","iopub.execute_input":"2023-05-21T09:42:55.221741Z","iopub.status.idle":"2023-05-21T09:43:30.759401Z","shell.execute_reply.started":"2023-05-21T09:42:55.221681Z","shell.execute_reply":"2023-05-21T09:43:30.758467Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:bewhvlbr) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d42d638a3ef409f95e567056ba72709"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">crimson-meadow-29</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_3_F/runs/bewhvlbr' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_F/runs/bewhvlbr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230521_092303-bewhvlbr/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:bewhvlbr). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_094255-8cxkag28</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_3_F/runs/8cxkag28' target=\"_blank\">pleasant-darkness-30</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_3_F' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshvrma/Assignment_3_F' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_F</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshvrma/Assignment_3_F/runs/8cxkag28' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_F/runs/8cxkag28</a>"},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/harshvrma/Assignment_3_F/runs/8cxkag28?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7b71601c6ad0>"},"metadata":{}}]},{"cell_type":"code","source":"def train():\n    var1 = wandb.init()\n    var2 = var1.config\n    embedding_size_input = var2.embedding_size\n    dropout_enc_dec = var2.dropout\n    cell_typeval = var2.cell_type\n    hidden_size_encdec = var2.hidden_size\n    enc_num_layers = var2.enc_num_layers\n    number_of_epochs = var2.epochs\n    batch_size = var2.batch_size\n    beam_width = var2.beam_search\n    training(input_size_encoder ,embedding_size_input, dropout_enc_dec, cell_typeval,hidden_size_encdec, enc_num_layers,  enc_num_layers,number_of_epochs ,num_decoder_tokens,num_decoder_tokens,batch_size,beam_width)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:43:30.764339Z","iopub.execute_input":"2023-05-21T09:43:30.766767Z","iopub.status.idle":"2023-05-21T09:43:30.776288Z","shell.execute_reply.started":"2023-05-21T09:43:30.766726Z","shell.execute_reply":"2023-05-21T09:43:30.775049Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"training(input_size_encoder ,128, 0.2, \"LSTM\",128, 2, 2,10,num_decoder_tokens,num_decoder_tokens,32,1)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:43:30.778078Z","iopub.execute_input":"2023-05-21T09:43:30.778933Z","iopub.status.idle":"2023-05-21T09:58:34.914437Z","shell.execute_reply.started":"2023-05-21T09:43:30.778894Z","shell.execute_reply":"2023-05-21T09:58:34.913385Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[Epoch 0 / 10]\n0.00146484375\n[Epoch 1 / 10]\n0.08740234375\n[Epoch 2 / 10]\n0.180419921875\n[Epoch 3 / 10]\n0.23291015625\n[Epoch 4 / 10]\n0.288330078125\n[Epoch 5 / 10]\n0.326171875\n[Epoch 6 / 10]\n0.34912109375\n[Epoch 7 / 10]\n0.356201171875\n[Epoch 8 / 10]\n0.370361328125\n[Epoch 9 / 10]\n0.390380859375\n","output_type":"stream"}]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n    'parameters': {'embedding_size': {'values': [128, 256, 512]},\n                   'epochs' :{'values':[10,20,30,40]},\n                   'hidden_size': {'values': [128, 256, 512]},\n                   'batch_size': {'values': [128,256,512]},\n                   'enc_num_layers': {'values': [1,2,3]},\n                   'dropout': {'values': [0.1, 0.2, 0.3, 0.4]},\n                   'dec_num_layers':{'values': [1,2,3]},\n                   'cell_type': {'values': ['LSTM','GRU','RNN']},\n                   'beam_search':{'values':[1,2,3,4,5]}\n                }}","metadata":{"execution":{"iopub.status.busy":"2023-05-21T10:30:10.542334Z","iopub.execute_input":"2023-05-21T10:30:10.543300Z","iopub.status.idle":"2023-05-21T10:30:10.550589Z","shell.execute_reply.started":"2023-05-21T10:30:10.543261Z","shell.execute_reply":"2023-05-21T10:30:10.549598Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"Assignment_3_Fin\")\nwandb.agent(sweep_id, train, count=50)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T10:30:13.790484Z","iopub.execute_input":"2023-05-21T10:30:13.790861Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: bfuntv20\nSweep URL: https://wandb.ai/harshvrma/Assignment_3_Fin/sweeps/bfuntv20\n","output_type":"stream"},{"name":"stderr","text":"wandb: Waiting for W&B process to finish... (success).\nwandb: / 0.048 MB of 0.048 MB uploaded (0.000 MB deduped)\nwandb: Run history:\nwandb: val_accuracy ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\nwandb: \nwandb: Run summary:\nwandb: val_accuracy 39.03809\nwandb: \nwandb: üöÄ View run pleasant-darkness-30 at: https://wandb.ai/harshvrma/Assignment_3_F/runs/8cxkag28\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20230521_094255-8cxkag28/logs\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ypdg5ddw with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_103022-ypdg5ddw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_3_Fin/runs/ypdg5ddw' target=\"_blank\">silvery-sweep-1</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_3_Fin' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_3_Fin/sweeps/bfuntv20' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_Fin/sweeps/bfuntv20</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshvrma/Assignment_3_Fin' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_Fin</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/harshvrma/Assignment_3_Fin/sweeps/bfuntv20' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_Fin/sweeps/bfuntv20</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshvrma/Assignment_3_Fin/runs/ypdg5ddw' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_3_Fin/runs/ypdg5ddw</a>"},"metadata":{}},{"name":"stdout","text":"[Epoch 0 / 30]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}